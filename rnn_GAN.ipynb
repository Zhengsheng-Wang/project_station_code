{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_getTrainingDataset(strPart, fn_preprocess, strJobsDir, nReaders = 5, nReadThreads = 5, nParseTreads = 5, nShuffleBufferSize = 1000, nBatchSize = 32):\n",
    "    strSampleFiles = fn_getTrainingSampleFiles(strPart, strJobsDir)\n",
    "    oDataset = tf.data.Dataset.list_files(\"jobs/*/samples/positive/sections/\" + strPart + \"/samples.npy\")\n",
    "    oDataset = oDataset.interleave(lambda strSampleFile: tf.data.TextLineDataset(strSampleFile), cycle_length=nReaders, \n",
    "                                  num_parallel_calls=nReadThreads)\n",
    "    oDataset = oDataset.map(fn_preprocess, num_parallel_calls = nParseTreads)\n",
    "    oDataset = oDataset.shuffle(nShuffleBufferSize)\n",
    "    return oDataset.batch(nBatchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "g_nBatchSize = 5\n",
    "\n",
    "def fn_DLoss(tensorTrue, tensorPred):\n",
    "        tensorOnes, tensorReal = tensorTrue[:g_nBatchSize, :, :], tensorPred[:g_nBatchSize, :, :]\n",
    "        tensorZeros, tensorFake = tensorTrue[g_nBatchSize:, :, :], tensorPred[g_nBatchSize:, :, :]\n",
    "        tensorLossReal = math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = tensorReal, labels = tensorOnes), 1)\n",
    "        tensorLossFake = math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = tensorFake, labels = tensorZeros), 1)\n",
    "        tensorLoss = tensorLossReal + tensorLossFake\n",
    "        return math.reduce_mean(tensorLoss)  \n",
    "def fn_GLoss(tensorTrue, tensorPred):\n",
    "    tensorLoss = math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = tensorPred, labels = tensorTrue), 1)\n",
    "    return math.reduce_mean(tensorLoss)\n",
    "\n",
    "class GANOperator(object):\n",
    "    def __init__(self, nLatentDim, nHiddenDimG, nHiddenDimD, nSeqLength, nBatchSize, nFeatures, npNArrSample):\n",
    "        self.nLatentDim = nLatentDim\n",
    "        self.nHiddenDimG = nHiddenDimG\n",
    "        self.nHiddenDimD = nHiddenDimD\n",
    "        self.nSeqLength = nSeqLength\n",
    "        self.nBatchSize = nBatchSize\n",
    "        self.nFeatures = nFeatures\n",
    "        \n",
    "        self.npNArrSample = npNArrSample\n",
    "        \n",
    "        self.oSeqGenerator = None\n",
    "        self.oSeqDiscriminator = None\n",
    "        self.fn_makeGenerator()\n",
    "        self.fn_makeDiscriminator()\n",
    "        self.fn_makeDiscriminatorModel()\n",
    "        self.fn_makeAdversariaModel()\n",
    "        \n",
    "    def fn_makeGenerator(self):\n",
    "        if self.oSeqGenerator:\n",
    "            return self.oSeqGenerator\n",
    "        self.oSeqGenerator = Sequential()\n",
    "        self.oSeqGenerator.add(GRU(self.nHiddenDimG, dropout = 0.1, recurrent_dropout = 0.5, return_sequences = True))\n",
    "        self.oSeqGenerator.add(Dense(self.nFeatures, activation = math.tanh))\n",
    "    def fn_makeDiscriminator(self):\n",
    "        if self.oSeqDiscriminator:\n",
    "            return self.oSeqDiscriminator\n",
    "        self.oSeqDiscriminator = Sequential()\n",
    "        self.oSeqDiscriminator.add(GRU(self.nHiddenDimD, dropout = 0.1, recurrent_dropout = 0.5, return_sequences = True))\n",
    "        self.oSeqDiscriminator.add(Dense(1))\n",
    "    \n",
    "    def fn_makeDiscriminatorModel(self):\n",
    "        self.oSeqDiscriminatorModel = Sequential()\n",
    "        self.oSeqDiscriminatorModel.add(self.oSeqDiscriminator)\n",
    "        oOptimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "        self.oSeqDiscriminatorModel.compile(loss=fn_DLoss, optimizer=oOptimizer, metrics=[\"accuracy\"])\n",
    "    def fn_makeAdversariaModel(self):\n",
    "        self.oSeqAdversarialModel = Sequential()\n",
    "        self.oSeqAdversarialModel.add(self.oSeqGenerator)\n",
    "        self.oSeqAdversarialModel.add(self.oSeqDiscriminator)\n",
    "        oOptimer = tf.optimizers.Adam()\n",
    "        self.oSeqAdversarialModel.compile(loss = fn_GLoss, optimizer=oOptimer, metrics=[\"accuracy\"])\n",
    "    \n",
    "    def fn_train(self, nEpochs = 2000):\n",
    "        for epoch in range(nEpochs):\n",
    "            npNArrSample = self.npNArrSample[np.random.randint(0, self.npNArrSample.shape[0], size = self.nBatchSize), :, :]\n",
    "            npNoise = np.random.uniform(-1.0, 1.0, size = [self.nBatchSize, self.nSeqLength, self.nLatentDim])\n",
    "            npNArrFake = self.oSeqGenerator.predict(npNoise)\n",
    "            npNArrX = np.concatenate((npNArrSample, npNArrFake))\n",
    "            npNArrY = np.ones([2 * self.nBatchSize, self.nSeqLength, 1])\n",
    "            npNArrY[self.nBatchSize:, :, :] = 0\n",
    "            fLossD = self.oSeqDiscriminatorModel.train_on_batch(npNArrX, npNArrY)\n",
    "            \n",
    "            npNArrY = np.ones([self.nBatchSize, self.nSeqLength, 1])\n",
    "            npNoise = np.random.uniform(-1.0, 1.0, size = [self.nBatchSize, self.nSeqLength, self.nLatentDim])\n",
    "            fLossA = self.oSeqAdversarialModel.train_on_batch(npNoise, npNArrY)\n",
    "\n",
    "            strMsg = \"%d: [D loss: %f, acc: %f]\" % (epoch, fLossD[0], fLossD[1])\n",
    "            strMsg = \"%s [A loss: %f, acc: %f]\" % (strMsg, fLossA[0], fLossA[1])\n",
    "            print(strMsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "npNArrSample = fn_getSamples(False, \"input\", \"jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "oGANOperator = GANOperator(nLatentDim=13, nHiddenDimG=100, nHiddenDimD=100, nSeqLength=10, nBatchSize=g_BatchSize, \n",
    "                           nFeatures=3, npNArrSample=npNArrSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 1.514886, acc: 0.880000] [A loss: 0.881162, acc: 0.000000]\n",
      "1: [D loss: 0.627459, acc: 1.000000] [A loss: 1.197199, acc: 0.000000]\n",
      "2: [D loss: 0.455534, acc: 1.000000] [A loss: 1.629318, acc: 0.000000]\n",
      "3: [D loss: 0.467883, acc: 0.990000] [A loss: 2.484230, acc: 0.000000]\n",
      "4: [D loss: 0.309302, acc: 1.000000] [A loss: 2.279864, acc: 0.000000]\n",
      "5: [D loss: 0.256230, acc: 1.000000] [A loss: 3.116179, acc: 0.000000]\n",
      "6: [D loss: 0.245415, acc: 1.000000] [A loss: 3.506050, acc: 0.000000]\n",
      "7: [D loss: 0.186093, acc: 1.000000] [A loss: 3.124023, acc: 0.000000]\n",
      "8: [D loss: 0.211709, acc: 1.000000] [A loss: 4.235771, acc: 0.000000]\n",
      "9: [D loss: 0.217565, acc: 1.000000] [A loss: 4.427356, acc: 0.000000]\n"
     ]
    }
   ],
   "source": [
    "oGANOperator.fn_train(nEpochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
