{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs _Building_ and _Training_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess training samples of a section\n",
    "\n",
    "Construct MinMaxScaler using training set, then use this MinMaxScaler transform training set and test set putting\n",
    "\n",
    "them into corresponding \"preprocessed\" directory. \n",
    "\n",
    "After preprocessing, the structure of jobs directory would be like below:\n",
    "\n",
    "- JOBXXXX\n",
    "    - status\n",
    "        - parts\n",
    "            - part\n",
    "                - section\n",
    "                    - samples\n",
    "                        - normal\n",
    "                            - train\n",
    "                                - status.csv\n",
    "                                - preprocessed\n",
    "                                    - status.csv\n",
    "                            - test\n",
    "                                - status.csv\n",
    "                                - preprocessed\n",
    "                                    - status.csv\n",
    "        - raw\n",
    "        - valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_makeMinMaxScaler(strPart, strSection, strJobsDir):\n",
    "    oMinMaxScaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    pdDfSectionsTrainSamples = fn_getSectionsTrainSamples(strPart, strSection, strJobsDir)\n",
    "    oMinMaxScaler.fit(pdDfSectionsTrainSamples.values)\n",
    "    return oMinMaxScaler\n",
    "def fn_getSectionsTrainSamples(strPart, strSection, strJobsDir):\n",
    "    listPdDfSectionTrainSamples = []\n",
    "    for job in os.listdir(strJobsDir):\n",
    "        strJobDir = os.path.join(strJobsDir, job)\n",
    "        listStrStatusDirs = [os.path.join(strJobDir, name) for name in os.listdir(strJobDir) if \"Demod\" in name]\n",
    "        for strStatusDir in listStrStatusDirs:\n",
    "            strSectionTrainSamplesFile = os.path.join(strStatusDir, \"parts/\" + strPart + \"/\" + strSection\n",
    "                                                    + \"/samples/normal/train/samples.csv\")\n",
    "            pdDfSectionTrainSamples = pd.read_csv(strSectionTrainSamplesFile)\n",
    "            listPdDfSectionTrainSamples.append(pdDfSectionTrainSamples)\n",
    "    pdDfSectionsTrainSamples = pd.concat(listPdDfSectionTrainSamples)\n",
    "    return pdDfSectionsTrainSamples\n",
    "        \n",
    "def fn_preprocessSectionTrainSamples(strPart, strSection, oMinMaxScaler, strJobsDir):\n",
    "    for job in os.listdir(strJobsDir):\n",
    "        strJobDir = os.path.join(strJobsDir, job)\n",
    "        listStrStatusDirs = [os.path.join(strJobDir, name) for name in os.listdir(strJobDir) if \"Demod\" in name]\n",
    "        for strStatusDir in listStrStatusDirs:\n",
    "            strSectionTrainSamplesFile = os.path.join(strStatusDir, \"parts/\" + strPart + \"/\" + strSection +\n",
    "                                                    \"/samples/normal/train/samples.csv\")\n",
    "            pdDfSectionTrainSamples = pd.read_csv(strSectionTrainSamplesFile)\n",
    "            strPreprocessedSectionTrainSamplesDir = os.path.join(os.path.split(strSectionTrainSamplesFile)[0], \"preprocessed\")\n",
    "            if os.path.exists(strPreprocessedSectionTrainSamplesDir):\n",
    "                shutil.rmtree(strPreprocessedSectionTrainSamplesDir)\n",
    "            os.mkdir(strPreprocessedSectionTrainSamplesDir)\n",
    "            \n",
    "            \"\"\"\n",
    "            Since empty section train samples will cause a error in oMinMaxScaler, check it first.\n",
    "            If section train samples is emtpy, the corresponding preprocessed directory contains \n",
    "            an empty dataframe with same columns.\n",
    "            \"\"\"\n",
    "            if not pdDfSectionTrainSamples.empty:\n",
    "                pdDfPreprocessedSectionTrainSamples = \\\n",
    "                pd.DataFrame(data = oMinMaxScaler.transform(pdDfSectionTrainSamples.values), \n",
    "                             columns = pdDfSectionTrainSamples.columns)\n",
    "            else:\n",
    "                pdDfPreprocessedSectionTrainSamples = \\\n",
    "                pd.DataFrame(columns=pdDfSectionTrainSamples.columns)\n",
    "                \n",
    "            strPreprocessedSectionTrainSamplesFile = os.path.join(strPreprocessedSectionTrainSamplesDir, \n",
    "                                                                 \"samples.csv\")\n",
    "            pdDfPreprocessedSectionTrainSamples.to_csv(strPreprocessedSectionTrainSamplesFile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_splitCsvLine(strLine):\n",
    "    tensorDefs = [0.] * g_nFeatures\n",
    "    tensorRecord = tf.io.decode_csv(strLine, record_defaults=tensorDefs)\n",
    "    tensorX = tf.stack(tensorRecord)\n",
    "    return tensorX\n",
    "def fn_getRealDataset(strPart, strSection, strJobsDir, \n",
    "                      nReaders = 5, nReadThreads = 5, nParseTreads = 5, nShuffleBufferSize = 1000):\n",
    "    oDataset = tf.data.Dataset.list_files(strJobsDir + \"/*/*/parts/\" + strPart + \"/\"\n",
    "                                         + strSection + \"/samples/normal/train/preprocessed/samples.csv\")\n",
    "    oDataset = oDataset.interleave(lambda strSamplesFile: tf.data.TextLineDataset(strSamplesFile).skip(1), cycle_length=nReaders, \n",
    "                                  num_parallel_calls=nReadThreads)\n",
    "    oDataset = oDataset.map(fn_splitCsvLine, nParseTreads)\n",
    "    oDataset = oDataset.shuffle(nShuffleBufferSize)\n",
    "    return oDataset.batch(g_nBatchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "oMinMaxScalerInput = fn_makeMinMaxScaler(\"framelock\", \"input\", \"../jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_preprocessSectionTrainSamples(\"framelock\", \"input\", oMinMaxScaler, \"../jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set global hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nBatchSize = 250\n",
    "g_nFeatures = 3\n",
    "oDatasetFramelockInput = fn_getRealDataset(\"framelock\", \"input\", \"../jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputGAN(object):\n",
    "    def __init__(self):\n",
    "        self.fn_makeGenerator()\n",
    "        self.fn_makeDiscriminator()\n",
    "        \n",
    "    def fn_makeGenerator(self):\n",
    "        self.oSeqGe = tf.keras.Sequential(name=\"Ge\")\n",
    "        self.oSeqGe.add(tf.keras.layers.Dense(2, activation = tf.keras.activations.relu))\n",
    "        self.oSeqGe.add(tf.keras.layers.Dense(2, activation = tf.keras.activations.relu))\n",
    "        \n",
    "        self.oSeqGd = tf.keras.Sequential(name=\"Gd\")\n",
    "        self.oSeqGd.add(tf.keras.layers.Dense(2, activation = tf.keras.activations.relu))\n",
    "        self.oSeqGd.add(tf.keras.layers.Dense(3, activation = tf.keras.activations.sigmoid))\n",
    "        \n",
    "        self.oSeqGe1 = tf.keras.Sequential(name=\"Ge1\")\n",
    "        self.oSeqGe1.add(tf.keras.layers.Dense(2, activation = tf.keras.activations.relu))\n",
    "        self.oSeqGe1.add(tf.keras.layers.Dense(2, activation = tf.keras.activations.relu))\n",
    "    def fn_makeDiscriminator(self):\n",
    "        self.oSeqDInner = tf.keras.Sequential(name=\"DInner\")\n",
    "        self.oSeqDInner.add(tf.keras.layers.Dense(2, activation = tf.keras.activations.relu))\n",
    "        self.oSeqDInner.add(tf.keras.layers.Dense(2, activation = tf.keras.activations.relu))\n",
    "        \n",
    "        self.oSeqD = tf.keras.Sequential([\n",
    "            self.oSeqDInner,\n",
    "            tf.keras.layers.Dense(1, activation = tf.keras.activations.sigmoid)\n",
    "        ], name=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "oInputGAN = InputGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strPrefix = time.strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "strPrefix = \"epoch=20_batchsize=250_relu_change\"\n",
    "oSummaryWriterInputDLoss = tf.summary.create_file_writer(\"logs/input/\" + strPrefix + \"D\")\n",
    "oSummaryWriterInputALoss = tf.summary.create_file_writer(\"logs/input/\" + strPrefix + \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "oOptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "nStep = 0\n",
    "for nEphoch in range(20):\n",
    "    for tensorBatch in oDatasetFramelockInput:\n",
    "        #if nStep % 100 == 0:\n",
    "        #   print(nStep)\n",
    "        # Train discriminator\n",
    "        with tf.GradientTape() as oGradientTapeD:\n",
    "            # Generate fake samples\n",
    "            tensorEncodedSamples = oInputGAN.oSeqGe(tensorBatch)\n",
    "            tensorFakeSamples = oInputGAN.oSeqGd(tensorEncodedSamples)\n",
    "        \n",
    "            tensorTrainingSamples = tf.concat([tensorBatch, tensorFakeSamples], axis=0)\n",
    "            tensorPreds = oInputGAN.oSeqD(tensorTrainingSamples)\n",
    "            tensorLabels = tf.constant([[1.]] * tensorBatch.shape[0] + [[0.]] * tensorBatch.shape[0])\n",
    "            tensorDLoss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tensorLabels, tensorPreds))\n",
    "        listDGradients = oGradientTapeD.gradient(tensorDLoss, oInputGAN.oSeqD.trainable_variables)\n",
    "        oOptimizer.apply_gradients(zip(listDGradients, oInputGAN.oSeqD.trainable_variables))\n",
    "        with oSummaryWriterInputDLoss.as_default():\n",
    "            tf.summary.scalar(\"D_loss\", tensorDLoss, nStep)\n",
    "        \n",
    "        # Train genertor\n",
    "        with tf.GradientTape() as oGradientTapeG:\n",
    "            # Apperant loss\n",
    "            tensorEncodedSamples = oInputGAN.oSeqGe(tensorBatch)\n",
    "            tensorFakeSamples = oInputGAN.oSeqGd(tensorEncodedSamples)\n",
    "            tensorApperantLoss = tf.reduce_mean(tf.losses.mean_absolute_error(tensorBatch, tensorFakeSamples))\n",
    "            \n",
    "            # Latent loss\n",
    "            tensorEncoded1Samples = oInputGAN.oSeqGe1(tensorFakeSamples)\n",
    "            tensorLatentLoss = \\\n",
    "            tf.reduce_mean(tf.losses.mean_squared_error(tensorEncodedSamples, tensorEncoded1Samples))\n",
    "            \n",
    "            # Feature matching loss\n",
    "            tensorPredFeatures = oInputGAN.oSeqDInner(tensorFakeSamples)\n",
    "            tensorTrueFeatures = oInputGAN.oSeqDInner(tensorBatch)\n",
    "            tensorFeatureLoss = tf.reduce_mean(tf.losses.mean_squared_error(tensorPredFeatures, tensorTrueFeatures))\n",
    "            \n",
    "            tensorGLoss = tensorApperantLoss + tensorLatentLoss + tensorFeatureLoss\n",
    "            \n",
    "        # Insert all generator trainable variables into a list\n",
    "        listTrainableGVariables = []\n",
    "        listTrainableGVariables.extend(oInputGAN.oSeqGe.trainable_variables)\n",
    "        listTrainableGVariables.extend(oInputGAN.oSeqGd.trainable_variables)\n",
    "        listTrainableGVariables.extend(oInputGAN.oSeqGe1.trainable_variables)\n",
    "        \n",
    "        listGGradients = oGradientTapeG.gradient(tensorGLoss, listTrainableGVariables)\n",
    "        oOptimizer.apply_gradients(zip(listGGradients, listTrainableGVariables))\n",
    "        with oSummaryWriterInputALoss.as_default():\n",
    "            tf.summary.scalar(\"A_loss\", tensorGLoss, nStep)\n",
    "            \n",
    "        nStep = nStep + 1\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
