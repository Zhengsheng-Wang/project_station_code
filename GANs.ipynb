{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs _Building_ and _Training_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess training samples of a section\n",
    "\n",
    "Preprocess the samples from __jobs/JOBXXXX/samples/sections/section/samples.csv__ \n",
    "and put them into __jobs/JOBXXXX/samples/sections/section/preprocessed/samples.csv__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_preprocessJobsSectionSamples(strPart, strJobsDir, strJobsSamplesDir):\n",
    "    strJobsSectionSamplesDir = os.path.join(strJobsSamplesDir, \"sections/\" + strPart)\n",
    "    oMinMaxScaler = fn_makeScaler(strJobsSectionSamplesDir)\n",
    "    \n",
    "    for name in os.listdir(strJobsDir):\n",
    "        strJobDir = os.path.join(strJobsDir, name)\n",
    "        fn_preprocessJobSectionSamples(strPart, oMinMaxScaler, strJobDir)\n",
    "    return oMinMaxScaler\n",
    "    \n",
    "def fn_makeScaler(strJobsSectionSamplesDir):\n",
    "    strJobsSectionSamplesFile = os.path.join(strJobsSectionSamplesDir, \"samples.csv\")\n",
    "    pdDfJobsSectionSamples = pd.read_csv(strJobsSectionSamplesFile)\n",
    "    oMinMaxScaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    oMinMaxScaler.fit(pdDfJobsSectionSamples.values)\n",
    "    return oMinMaxScaler\n",
    "def fn_preprocessJobSectionSamples(strPart, oMinMaxScaler, strJobDir):\n",
    "    strSectionSamplesFile = os.path.join(strJobDir, \"samples/sections/\" + strPart + \"/samples.csv\")\n",
    "    pdDfSectionSamples = pd.read_csv(strSectionSamplesFile)\n",
    "    npNArrPreprocessedSectionSamples = oMinMaxScaler.transform(pdDfSectionSamples.values)\n",
    "    pdDfPreprocessedSectionSamples = pd.DataFrame(npNArrPreprocessedSectionSamples, columns=pdDfSectionSamples.columns)\n",
    "    strPreprocessedSectionSamplesDir = os.path.join(strJobDir, \"samples/sections/\" + strPart + \"/preprocessed\")\n",
    "    if os.path.exists(strPreprocessedSectionSamplesDir):\n",
    "        shutil.rmtree(strPreprocessedSectionSamplesDir)\n",
    "    os.mkdir(strPreprocessedSectionSamplesDir)\n",
    "    strPreprocessedSectionSamplesFile = os.path.join(strPreprocessedSectionSamplesDir, \"samples.csv\")\n",
    "    pdDfPreprocessedSectionSamples.to_csv(strPreprocessedSectionSamplesFile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_splitCsvLine(strLine):\n",
    "    tensorDefs = [0.] * g_nFeatures\n",
    "    tensorRecord = tf.io.decode_csv(strLine, record_defaults=tensorDefs)\n",
    "    tensorX = tf.stack(tensorRecord)\n",
    "    return tensorX\n",
    "def fn_getRealDataset(strPart, strJobsDir, nReaders = 5, nReadThreads = 5, nParseTreads = 5, nShuffleBufferSize = 1000):\n",
    "    oDataset = tf.data.Dataset.list_files(strJobsDir + \"/*/samples/sections/\" + strPart + \"/preprocessed/samples.csv\")\n",
    "    oDataset = oDataset.interleave(lambda strSampleFile: tf.data.TextLineDataset(strSampleFile).skip(1), cycle_length=nReaders, \n",
    "                                  num_parallel_calls=nReadThreads)\n",
    "    oDataset = oDataset.map(fn_splitCsvLine, nParseTreads)\n",
    "    oDataset = oDataset.shuffle(nShuffleBufferSize)\n",
    "    return oDataset.batch(g_nBatchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set global hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nBatchSize = 1\n",
    "g_nFeatures = 3\n",
    "g_nCodingSize = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "oDataset = tf.data.Dataset.from_tensor_slices([[1, 3,  4], [3, 4, 19]])\n",
    "for i in oDataset:\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "oMinMaxScalerInput = fn_preprocessJobsSectionSamples(\"input\", \"jobs\", \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "oDatasetInputReal = fn_getRealDataset(\"input\", \"jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputGAN(object):\n",
    "    def __init__(self):\n",
    "        self.oSeqGenerator = None\n",
    "        self.oSeqDiscriminator = None\n",
    "        \n",
    "        self.fn_lossD = tf.losses.BinaryCrossentropy\n",
    "        self.fn_lossA = tf.losses.BinaryCrossentropy\n",
    "        self.fn_metricD = tf.metrics.BinaryAccuracy\n",
    "        self.fn_metricA = tf.metrics.BinaryAccuracy\n",
    "        \n",
    "        self.fn_makeGenerator()\n",
    "        self.fn_makeDiscriminator()\n",
    "        self.fn_makeDiscriminatorTrainingModel()\n",
    "        self.fn_makeAdversariaTrainingModel()\n",
    "        \n",
    "        \n",
    "    def fn_makeGenerator(self):\n",
    "        if self.oSeqGenerator:\n",
    "            return self.oSeqGenerator\n",
    "        self.oSeqGenerator = tf.keras.Sequential()\n",
    "        self.oSeqGenerator.add(tf.keras.layers.Dense(64, activation = tf.keras.activations.relu))\n",
    "        self.oSeqGenerator.add(tf.keras.layers.Dense(g_nFeatures, activation = tf.keras.activations.sigmoid))\n",
    "    def fn_makeDiscriminator(self):\n",
    "        if self.oSeqDiscriminator:\n",
    "            return self.oSeqDiscriminator\n",
    "        self.oSeqDiscriminator = tf.keras.Sequential()\n",
    "        self.oSeqDiscriminator.add(tf.keras.layers.Dense(64, activation = tf.keras.activations.relu))\n",
    "        self.oSeqDiscriminator.add(tf.keras.layers.Dense(1, activation = tf.keras.activations.sigmoid))\n",
    "    \n",
    "    def fn_makeDiscriminatorTrainingModel(self):\n",
    "        self.oSeqDiscriminatorModel = tf.keras.Sequential()\n",
    "        self.oSeqDiscriminatorModel.add(self.oSeqDiscriminator)\n",
    "        oOptimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        self.oSeqDiscriminatorModel.compile(loss=self.fn_lossD, optimizer=oOptimizer, metrics=[self.fn_metricD])\n",
    "    def fn_makeAdversariaTrainingModel(self):\n",
    "        self.oSeqAdversarialModel = tf.keras.Sequential()\n",
    "        self.oSeqAdversarialModel.add(self.oSeqGenerator)\n",
    "        self.oSeqAdversarialModel.add(self.oSeqDiscriminator)\n",
    "        oOptimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "        self.oSeqAdversarialModel.compile(loss = self.fn_lossA, optimizer=oOptimizer, metrics=[self.fn_metricA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "oInputGAN = InputGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/input/d_64_relu_1_sigmoid_loss\n",
    "!rm -rf logs/input/d_64_relu_1_sigmoid_acc\n",
    "!rm -rf logs/input/a_g_64_relu_3_sigmoid_loss\n",
    "!rm -rf logs/input/a_g_64_relu_3_sigmoid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "oSummaryWriterInputDLoss = tf.summary.create_file_writer(\"logs/input/d_64_relu_1_sigmoid_loss\")\n",
    "oSummaryWriterInputDAcc = tf.summary.create_file_writer(\"logs/input/d_64_relu_1_sigmoid_acc\")\n",
    "oSummaryWriterInputALoss = tf.summary.create_file_writer(\"logs/input/a_g_64_relu_3_sigmoid_loss\")\n",
    "oSummaryWriterInputAAcc = tf.summary.create_file_writer(\"logs/input/a_g_64_relu_3_sigmoid_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "name for name_scope must be a string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-3707acf67685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtensorLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg_nBatchSize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg_nBatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moInputGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moSeqDiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mfDLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfDAcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moInputGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moSeqDiscriminatorModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorRealAndFakeSamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moSummaryWriterInputDLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"discriminator_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfDLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnEphoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    990\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[1;32m    991\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2431\u001b[0m     \u001b[0mis_compile_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2433\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2434\u001b[0m       \u001b[0mis_compile_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_compile_from_inputs\u001b[0;34m(self, all_inputs, target, orig_inputs, orig_target)\u001b[0m\n\u001b[1;32m   2666\u001b[0m         \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_weight_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m         \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2668\u001b[0;31m         experimental_run_tf_function=self._experimental_run_tf_function)\n\u001b[0m\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m   \u001b[0;31m# TODO(omalleyt): Consider changing to a more descriptive function name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m           \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mskip_target_masks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_skip_target_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m           masks=self._prepare_output_masks())\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m       \u001b[0;31m# Prepare sample weight modes. List with the same length as model outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_handle_metrics\u001b[0;34m(self, outputs, targets, skip_target_masks, sample_weights, masks, return_weighted_metrics, return_weighted_and_unweighted_metrics)\u001b[0m\n\u001b[1;32m   2061\u001b[0m           metric_results.extend(\n\u001b[1;32m   2062\u001b[0m               self._handle_per_output_metrics(self._per_output_metrics[i],\n\u001b[0;32m-> 2063\u001b[0;31m                                               target, output, output_mask))\n\u001b[0m\u001b[1;32m   2064\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_weighted_and_unweighted_metrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreturn_weighted_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m           metric_results.extend(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_handle_per_output_metrics\u001b[0;34m(self, metrics_dict, y_true, y_pred, mask, weights)\u001b[0m\n\u001b[1;32m   2010\u001b[0m     \u001b[0mmetric_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2012\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2013\u001b[0m         metric_result = training_utils.call_metric_function(\n\u001b[1;32m   2014\u001b[0m             metric_fn, y_true, y_pred, weights=weights, mask=mask)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mname_scope\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0mName\u001b[0m \u001b[0mscope\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m   \"\"\"\n\u001b[0;32m--> 765\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6420\u001b[0m     \"\"\"\n\u001b[1;32m   6421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6422\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name for name_scope must be a string.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6423\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6424\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exit_fns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: name for name_scope must be a string."
     ]
    }
   ],
   "source": [
    "nStep = 0\n",
    "for nEphoch in range(2000):\n",
    "    for tensorBatch in oDatasetInputReal:\n",
    "        tensorNoise = tf.random.uniform(shape=[g_nBatchSize,  g_nCodingSize])\n",
    "        tensorGeneratedSamples = oInputGAN.oSeqGenerator(tensorNoise)\n",
    "        tensorRealAndFakeSamples = tf.concat([tensorBatch, tensorGeneratedSamples], axis=0)\n",
    "        tensorLabels = tf.constant([[1.]] * g_nBatchSize + [[0.]] * g_nBatchSize)\n",
    "        oInputGAN.oSeqDiscriminator.trainable = True\n",
    "        fDLoss, fDAcc = oInputGAN.oSeqDiscriminatorModel.train_on_batch(tensorRealAndFakeSamples, tensorLabels)\n",
    "        with oSummaryWriterInputDLoss.as_default():\n",
    "            tf.summary.scalar(\"discriminator_loss\", fDLoss, nEphoch)\n",
    "        with oSummaryWriterInputDAcc.as_default():\n",
    "            tf.summary.scalar(\"discriminator_acc\", fDAcc, nEphoch)\n",
    "        \n",
    "        tensorNoise = tf.random.uniform(shape=[2 * g_nBatchSize,  g_nCodingSize])\n",
    "        tensorLabels = tf.constant([[0.]] * 2 * g_nBatchSize)\n",
    "        oInputGAN.oSeqDiscriminator.trainable = False\n",
    "        fALoss, fAAcc = oInputGAN.oSeqAdversarialModel.train_on_batch(tensorNoise, tensorLabels)\n",
    "        with oSummaryWriterInputALoss.as_default():\n",
    "            tf.summary.scalar(\"adversarial_loss\", fALoss, nEphoch)\n",
    "        with oSummaryWriterInputAAcc.as_default():\n",
    "            tf.summary.scalar(\"adversarial_acc\", fAAcc, nEphoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
