{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs _Building_ and _Training_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess training samples of a section\n",
    "\n",
    "Preprocess the samples from __jobs/JOBXXXX/samples/sections/section/samples.csv__ \n",
    "and put them into __jobs/JOBXXXX/samples/sections/section/preprocessed/samples.csv__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_preprocessJobsSectionSamples(strPart, strJobsDir, strJobsSamplesDir):\n",
    "    strJobsSectionSamplesDir = os.path.join(strJobsSamplesDir, \"sections/\" + strPart)\n",
    "    oMinMaxScaler = fn_makeScaler(strJobsSectionSamplesDir)\n",
    "    \n",
    "    for name in os.listdir(strJobsDir):\n",
    "        strJobDir = os.path.join(strJobsDir, name)\n",
    "        fn_preprocessJobSectionSamples(strPart, oMinMaxScaler, strJobDir)\n",
    "    return oMinMaxScaler\n",
    "    \n",
    "def fn_makeScaler(strJobsSectionSamplesDir):\n",
    "    strJobsSectionSamplesFile = os.path.join(strJobsSectionSamplesDir, \"samples.csv\")\n",
    "    pdDfJobsSectionSamples = pd.read_csv(strJobsSectionSamplesFile)\n",
    "    oMinMaxScaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    oMinMaxScaler.fit(pdDfJobsSectionSamples.values)\n",
    "    return oMinMaxScaler\n",
    "def fn_preprocessJobSectionSamples(strPart, oMinMaxScaler, strJobDir):\n",
    "    strSectionSamplesFile = os.path.join(strJobDir, \"samples/sections/\" + strPart + \"/samples.csv\")\n",
    "    pdDfSectionSamples = pd.read_csv(strSectionSamplesFile)\n",
    "    npNArrPreprocessedSectionSamples = oMinMaxScaler.transform(pdDfSectionSamples.values)\n",
    "    pdDfPreprocessedSectionSamples = pd.DataFrame(npNArrPreprocessedSectionSamples, columns=pdDfSectionSamples.columns)\n",
    "    strPreprocessedSectionSamplesDir = os.path.join(strJobDir, \"samples/sections/\" + strPart + \"/preprocessed\")\n",
    "    if os.path.exists(strPreprocessedSectionSamplesDir):\n",
    "        shutil.rmtree(strPreprocessedSectionSamplesDir)\n",
    "    os.mkdir(strPreprocessedSectionSamplesDir)\n",
    "    strPreprocessedSectionSamplesFile = os.path.join(strPreprocessedSectionSamplesDir, \"samples.csv\")\n",
    "    pdDfPreprocessedSectionSamples.to_csv(strPreprocessedSectionSamplesFile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_splitCsvLine(strLine):\n",
    "    tensorDefs = [0.] * g_nFeatures\n",
    "    tensorRecord = tf.io.decode_csv(strLine, record_defaults=tensorDefs)\n",
    "    tensorX = tf.stack(tensorRecord)\n",
    "    return tensorX\n",
    "def fn_getRealDataset(strPart, strJobsDir, nReaders = 5, nReadThreads = 5, nParseTreads = 5, nShuffleBufferSize = 1000):\n",
    "    oDataset = tf.data.Dataset.list_files(strJobsDir + \"/*/samples/sections/\" + strPart + \"/preprocessed/samples.csv\")\n",
    "    oDataset = oDataset.interleave(lambda strSampleFile: tf.data.TextLineDataset(strSampleFile).skip(1), cycle_length=nReaders, \n",
    "                                  num_parallel_calls=nReadThreads)\n",
    "    oDataset = oDataset.map(fn_splitCsvLine, nParseTreads)\n",
    "    oDataset = oDataset.shuffle(nShuffleBufferSize)\n",
    "    return oDataset.batch(g_nBatchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set global hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nBatchSize = 1\n",
    "g_nFeatures = 3\n",
    "g_nCodingSize = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "oMinMaxScalerInput = fn_preprocessJobsSectionSamples(\"input\", \"jobs\", \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "oDatasetInputReal = fn_getRealDataset(\"input\", \"jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputGAN(object):\n",
    "    def __init__(self):\n",
    "        self.fn_lossD = tf.losses.binary_crossentropy\n",
    "        self.fn_lossA = tf.losses.binary_crossentropy\n",
    "        self.fn_metricD = tf.metrics.binary_accuracy\n",
    "        self.fn_metricA = tf.metrics.binary_accuracy\n",
    "        \n",
    "        self.fn_makeGenerator()\n",
    "        self.fn_makeDiscriminator()\n",
    "        self.fn_makeDiscriminatorTrainingModel()\n",
    "        self.fn_makeAdversariaTrainingModel()\n",
    "        \n",
    "    def fn_makeGenerator(self):\n",
    "        self.oSeqGenerator = tf.keras.Sequential()\n",
    "        self.oSeqGenerator.add(tf.keras.layers.Dense(64, activation = tf.keras.activations.relu))\n",
    "        self.oSeqGenerator.add(tf.keras.layers.Dense(g_nFeatures, activation = tf.keras.activations.sigmoid))\n",
    "    def fn_makeDiscriminator(self):\n",
    "        self.oSeqDiscriminator = tf.keras.Sequential()\n",
    "        self.oSeqDiscriminator.add(tf.keras.layers.Dense(64, activation = tf.keras.activations.relu))\n",
    "        self.oSeqDiscriminator.add(tf.keras.layers.Dense(1, activation = tf.keras.activations.sigmoid))\n",
    "    \n",
    "    def fn_makeDiscriminatorTrainingModel(self):\n",
    "        self.oSeqDiscriminatorModel = tf.keras.Sequential()\n",
    "        self.oSeqDiscriminatorModel.add(self.oSeqDiscriminator)\n",
    "        oOptimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        self.oSeqDiscriminatorModel.compile(loss=self.fn_lossD, optimizer=oOptimizer)\n",
    "    def fn_makeAdversariaTrainingModel(self):\n",
    "        self.oSeqAdversarialModel = tf.keras.Sequential()\n",
    "        self.oSeqAdversarialModel.add(self.oSeqGenerator)\n",
    "        self.oSeqAdversarialModel.add(self.oSeqDiscriminator)\n",
    "        oOptimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "        self.oSeqDiscriminator.trainable = False\n",
    "        self.oSeqAdversarialModel.compile(loss = self.fn_lossA, optimizer=oOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "oInputGAN = InputGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/input/d_64_relu_1_sigmoid_loss\n",
    "!rm -rf logs/input/a_g_64_relu_3_sigmoid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "oSummaryWriterInputDLoss = tf.summary.create_file_writer(\"logs/input/d_64_relu_1_sigmoid_loss\")\n",
    "oSummaryWriterInputALoss = tf.summary.create_file_writer(\"logs/input/a_g_64_relu_3_sigmoid_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "nStep = 0\n",
    "for nEphoch in range(10):\n",
    "    for tensorBatch in oDatasetInputReal:\n",
    "        tensorNoise = tf.random.normal(shape=[g_nBatchSize,  g_nCodingSize])\n",
    "        tensorGeneratedSamples = oInputGAN.oSeqGenerator(tensorNoise)\n",
    "        tensorRealAndFakeSamples = tf.concat([tensorBatch, tensorGeneratedSamples], axis=0)\n",
    "        tensorLabels = tf.constant([[1.]] * g_nBatchSize + [[0.]] * g_nBatchSize)\n",
    "        fDLoss = oInputGAN.oSeqDiscriminatorModel.train_on_batch(tensorRealAndFakeSamples, tensorLabels)\n",
    "        with oSummaryWriterInputDLoss.as_default():\n",
    "            tf.summary.scalar(\"discriminator_loss\", fDLoss, nStep)\n",
    "        '''\n",
    "        with oSummaryWriterInputDAcc.as_default():\n",
    "            tf.summary.scalar(\"discriminator_acc\", fDAcc, nStep)\n",
    "        '''\n",
    "        tensorNoise = tf.random.normal(shape=[g_nBatchSize,  g_nCodingSize])\n",
    "        tensorLabels = tf.constant([[1.]] * g_nBatchSize)\n",
    "        fALoss = oInputGAN.oSeqAdversarialModel.train_on_batch(tensorNoise, tensorLabels)\n",
    "        with oSummaryWriterInputALoss.as_default():\n",
    "            tf.summary.scalar(\"adversarial_loss\", fALoss, nStep)\n",
    "        '''\n",
    "        with oSummaryWriterInputAAcc.as_default():\n",
    "            tf.summary.scalar(\"adversarial_acc\", fAAcc, nStep)\n",
    "        '''\n",
    "        nStep += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=28609693, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[-0.07572332, -0.94126445,  0.08374636],\n",
       "       [-0.5398398 , -0.97281134,  1.2739346 ]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal(shape = [2, 3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
