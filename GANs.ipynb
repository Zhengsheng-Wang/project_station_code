{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate latent data\n",
    "Use np.random.uniform to generate latent data. And append a __label__ column to the latent data.\n",
    "\n",
    ">There are two kinds of latent data. \n",
    ">1. \"jobs/JOBXXXXX/samples/sections/section/latent/\"*\n",
    ">2. \"samples/sections/section/latent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_generateLatentSamples(strPart, strJobsDir):\n",
    "    for name in os.listdir(strJobsDir):\n",
    "        strJobDir = os.path.join(strJobsDir, name)\n",
    "        strSectionSamplesDir = os.path.join(strJobDir, \"samples/sections/\" + strPart)\n",
    "        \n",
    "        strLatentSectionSamplesDir = os.path.join(strSectionSamplesDir, \"latent\")\n",
    "        if os.path.exists(strLatentSectionSamplesDir):\n",
    "            shutil.rmtree(strLatentSectionSamplesDir)\n",
    "        os.mkdir(strLatentSectionSamplesDir)\n",
    "        strSectionSamplesFile = os.path.join(strSectionSamplesDir, \"samples.csv\")\n",
    "        pdDfSectionSamples = pd.read_csv(strSectionSamplesFile)\n",
    "        npNArrLatentSamples = np.random.uniform(0., 1., size = pdDfSectionSamples.shape)\n",
    "        pdDfLatentSectionSamples = pd.DataFrame(data = npNArrLatentSamples, \n",
    "                                                columns=pdDfSectionSamples.columns, index=pdDfSectionSamples.index)\n",
    "        pdDfLatentSectionSamples[\"label\"] = 0.\n",
    "        strLatentSectionSamplesFile = os.path.join(strLatentSectionSamplesDir, \"samples.csv\")\n",
    "        pdDfLatentSectionSamples.to_csv(strLatentSectionSamplesFile, index=False)\n",
    "        \n",
    "        strReservedLatentSectionSamplesDir = os.path.join(strLatentSectionSamplesDir, \"reserved\")\n",
    "        os.mkdir(strReservedLatentSectionSamplesDir)\n",
    "        npNArrReservedLatentSamples = np.random.uniform(0., 1., size = pdDfSectionSamples.shape)\n",
    "        pdDfReservedLatentSectionSamples = pd.DataFrame(data = npNArrReservedLatentSamples, \n",
    "                                               columns=pdDfSectionSamples.columns, index=pdDfSectionSamples.index)\n",
    "        pdDfReservedLatentSectionSamples[\"label\"]  = 0\n",
    "        strReservedLatentSectionSamplesFile = os.path.join(strReservedLatentSectionSamplesDir, \"samples.csv\")\n",
    "        pdDfReservedLatentSectionSamples.to_csv(strReservedLatentSectionSamplesFile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess training samples of a section\n",
    "\n",
    "Preprocess the samples from __jobs/JOBXXXXX/samples/sections/section/__ \n",
    "and put them into __jobs/JOBXXXX/samples/sections/section/preprocessed__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_preprocessJobsSectionSamples(strPart, strJobsDir, strJobsSamplesDir):\n",
    "    strJobsSectionSamplesDir = os.path.join(strJobsSamplesDir, \"sections/\" + strPart)\n",
    "    oMinMaxScaler = fn_makeScaler(strJobsSectionSamplesDir)\n",
    "    \n",
    "    for name in os.listdir(strJobsDir):\n",
    "        strJobDir = os.path.join(strJobsDir, name)\n",
    "        fn_preprocessJobSectionSamples(strPart, oMinMaxScaler, strJobDir)\n",
    "    return oMinMaxScaler\n",
    "    \n",
    "def fn_makeScaler(strJobsSectionSamplesDir):\n",
    "    strJobsSectionSamplesFile = os.path.join(strJobsSectionSamplesDir, \"samples.csv\")\n",
    "    pdDfJobsSectionSamples = pd.read_csv(strJobsSectionSamplesFile)\n",
    "    oMinMaxScaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    oMinMaxScaler.fit(pdDfJobsSectionSamples.values)\n",
    "    return oMinMaxScaler\n",
    "def fn_preprocessJobSectionSamples(strPart, oMinMaxScaler, strJobDir):\n",
    "    strSectionSamplesFile = os.path.join(strJobDir, \"samples/sections/\" + strPart + \"/samples.csv\")\n",
    "    pdDfSectionSamples = pd.read_csv(strSectionSamplesFile)\n",
    "    npNArrPreprocessedSectionSamples = oMinMaxScaler.transform(pdDfSectionSamples.values)\n",
    "    pdDfPreprocessedSectionSamples = pd.DataFrame(npNArrPreprocessedSectionSamples, columns=pdDfSectionSamples.columns)\n",
    "    pdDfPreprocessedSectionSamples[\"label\"] = 1.\n",
    "    strPreprocessedSectionSamplesDir = os.path.join(strJobDir, \"samples/sections/\" + strPart + \"/preprocessed\")\n",
    "    if os.path.exists(strPreprocessedSectionSamplesDir):\n",
    "        shutil.rmtree(strPreprocessedSectionSamplesDir)\n",
    "    os.mkdir(strPreprocessedSectionSamplesDir)\n",
    "    strPreprocessedSectionSamplesFile = os.path.join(strPreprocessedSectionSamplesDir, \"samples.csv\")\n",
    "    pdDfPreprocessedSectionSamples.to_csv(strPreprocessedSectionSamplesFile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nFeatures = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_getDiscriminatorTrainingDataset(strPart, fn_splitCsvLineIntoXAndY, strJobsDir, \n",
    "                          nReaders = 5, nReadThreads = 5, nParseTreads = 5, nShuffleBufferSize = 1000, nBatchSize = 32):\n",
    "    oDataset = tf.data.Dataset.list_files(strJobsDir + \"/*/samples/sections/\" + strPart + \"/*/samples.csv\")\n",
    "    oDataset = oDataset.interleave(lambda strSampleFile: tf.data.TextLineDataset(strSampleFile).skip(1), cycle_length=nReaders, \n",
    "                                  num_parallel_calls=nReadThreads)\n",
    "    oDataset = oDataset.map(fn_splitCsvLineIntoXAndY, nParseTreads)\n",
    "    oDataset = oDataset.shuffle(nShuffleBufferSize)\n",
    "    return oDataset.batch(nBatchSize)\n",
    "\n",
    "def fn_splitCsvLineIntoXAndY(strLine):\n",
    "    tensorDefs = [0.] * g_nFeatures + [tf.constant([], dtype=tf.float32)]\n",
    "    tensorRecord = tf.io.decode_csv(strLine, record_defaults=tensorDefs)\n",
    "    tensorX = tf.stack(tensorRecord[:-1])\n",
    "    tensorY = tf.stack(tensorRecord[-1])\n",
    "    return tensorX, tensorY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nFeatures = 3\n",
    "oDataset = fn_getDiscriminatorTrainingDataset(\"input\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_preprocessJobsSectionSamples(\"input\", \"jobs\", \"samples\")\n",
    "fn_generateLatentSamples(\"input\", \"jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "oMinMaxScalerInput = fn_normalizeJobsSectionSamples(strPart = \"input\", strJobsDir = \"jobs\", strJobsSamplesDir = \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_generateLatentSamples(\"input\", \"jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nInputBatchSize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for i in dataset:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def fn_DLoss(tensorTrue, tensorPred):\n",
    "        tensorOnes, tensorReal = tensorTrue[:g_nInputBatchSize, :], tensorPred[:g_nInputBatchSize, :]\n",
    "        tensorZeros, tensorFake = tensorTrue[g_nInputBatchSize:, :], tensorPred[g_nInputBatchSize:, :]\n",
    "        tensorLossReal = tf.math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = tensorReal, labels = tensorOnes), 1)\n",
    "        tensorLossFake = tf.math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = tensorFake, labels = tensorZeros), 1)\n",
    "        tensorLoss = tensorLossReal + tensorLossFake\n",
    "        return tf.math.reduce_mean(tensorLoss)  \n",
    "def fn_GLoss(tensorTrue, tensorPred):\n",
    "    tensorLoss = tf.math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = tensorPred, labels = tensorTrue), 1)\n",
    "    return tf.math.reduce_mean(tensorLoss)\n",
    "\n",
    "class InputGANOperator(object):\n",
    "    \"\"\"\n",
    "        specify the number of features to be dealt by gan\n",
    "    \"\"\"\n",
    "    def __init__(self, nLatentDim, nHiddenDimG, nHiddenDimD, nBatchSize, nFeatures):\n",
    "        self.nLatentDim = nLatentDim\n",
    "        self.nHiddenDimG = nHiddenDimG\n",
    "        self.nHiddenDimD = nHiddenDimD\n",
    "        self.nBatchSize = nBatchSize\n",
    "        self.nFeatures = nFeatures\n",
    "        \n",
    "        self.oSeqGenerator = None\n",
    "        self.oSeqDiscriminator = None\n",
    "        self.fn_makeGenerator()\n",
    "        self.fn_makeDiscriminator()\n",
    "        self.fn_makeDiscriminatorModel()\n",
    "        self.fn_makeAdversariaModel()\n",
    "        \n",
    "    def fn_makeGenerator(self):\n",
    "        if self.oSeqGenerator:\n",
    "            return self.oSeqGenerator\n",
    "        self.oSeqGenerator = Sequential()\n",
    "        self.oSeqGenerator.add(Dense(64, activation = tf.keras.activations.relu))\n",
    "        self.oSeqGenerator.add(Dense(3, activation = tf.keras.activations.sigmoid))\n",
    "    def fn_makeDiscriminator(self):\n",
    "        if self.oSeqDiscriminator:\n",
    "            return self.oSeqDiscriminator\n",
    "        self.oSeqDiscriminator = Sequential()\n",
    "        self.oSeqDiscriminator.add(Dense(64, activation = tf.keras.activations.relu))\n",
    "        self.oSeqDiscriminator.add(Dense(1, activation = tf.keras.activations.sigmoid))\n",
    "    \n",
    "    def fn_makeDiscriminatorTrainingModel(self):\n",
    "        self.oSeqDiscriminatorModel = Sequential()\n",
    "        self.oSeqDiscriminatorModel.add(self.oSeqDiscriminator)\n",
    "        oOptimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "        self.oSeqDiscriminatorModel.compile(loss=fn_DLoss, optimizer=oOptimizer, metrics=[\"accuracy\"])\n",
    "    def fn_makeAdversariaTrainingModel(self):\n",
    "        self.oSeqAdversarialModel = Sequential()\n",
    "        self.oSeqAdversarialModel.add(self.oSeqGenerator)\n",
    "        self.oSeqAdversarialModel.add(self.oSeqDiscriminator)\n",
    "        self.oSeqDiscriminator.trainable = False\n",
    "        oOptimer = tf.optimizers.Adam()\n",
    "        self.oSeqAdversarialModel.compile(loss = fn_GLoss, optimizer=oOptimer, metrics=[\"accuracy\"])\n",
    "    \n",
    "    def fn_train(self, npNArrSamples, nEpochs = 2000):\n",
    "        for epoch in range(nEpochs):\n",
    "            npNArrTrueSamples = npNArrSamples[np.random.randint(0, npNArrSamples.shape[0], size = self.nBatchSize), :]\n",
    "            npNoise = np.random.uniform(-1.0, 1.0, size = [self.nBatchSize, self.nLatentDim])\n",
    "            npNArrFake = self.oSeqGenerator.predict(npNoise)\n",
    "            npNArrX = np.concatenate((npNArrTrueSamples, npNArrFake))\n",
    "            npNArrY = np.ones([2 * self.nBatchSize, 1])\n",
    "            npNArrY[self.nBatchSize:, :] = 0\n",
    "            fLossD = self.oSeqDiscriminatorModel.train_on_batch(npNArrX, npNArrY)\n",
    "            \n",
    "            npNArrY = np.ones([self.nBatchSize, 1])\n",
    "            npNoise = np.random.uniform(-1.0, 1.0, size = [self.nBatchSize, self.nLatentDim])\n",
    "            fLossA = self.oSeqAdversarialModel.train_on_batch(npNoise, npNArrY)\n",
    "\n",
    "            strMsg = \"%d: [D loss: %f, acc: %f]\" % (epoch, fLossD[0], fLossD[1])\n",
    "            strMsg = \"%s [A loss: %f, acc: %f]\" % (strMsg, fLossA[0], fLossA[1])\n",
    "            print(strMsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "oGANOperator = GANOperator(nLatentDim=8, nHiddenDimG=16, nHiddenDimD=16, nBatchSize=g_nBatchSize, \n",
    "                           nFeatures=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 1.452051, acc: 0.400000] [A loss: 0.490367, acc: 0.200000]\n",
      "1: [D loss: 1.434356, acc: 0.500000] [A loss: 0.474354, acc: 0.400000]\n",
      "2: [D loss: 1.459817, acc: 0.400000] [A loss: 0.508515, acc: 0.000000]\n",
      "3: [D loss: 1.454618, acc: 0.500000] [A loss: 0.449057, acc: 0.800000]\n",
      "4: [D loss: 1.487272, acc: 0.300000] [A loss: 0.479009, acc: 0.400000]\n",
      "5: [D loss: 1.391144, acc: 0.800000] [A loss: 0.503868, acc: 0.200000]\n",
      "6: [D loss: 1.450166, acc: 0.500000] [A loss: 0.478879, acc: 0.400000]\n",
      "7: [D loss: 1.487112, acc: 0.400000] [A loss: 0.463928, acc: 0.800000]\n",
      "8: [D loss: 1.436899, acc: 0.500000] [A loss: 0.446789, acc: 0.600000]\n",
      "9: [D loss: 1.470820, acc: 0.300000] [A loss: 0.479095, acc: 0.400000]\n"
     ]
    }
   ],
   "source": [
    "oGANOperator.fn_train(npNArrSamples, nEpochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
