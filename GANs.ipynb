{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess training samples of a section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_normalizeJobsSectionSamples(strPart, strJobsDir, strJobsSamplesDir):\n",
    "    strJobsSamplesFile = os.path.join(strJobsSamplesDir, \"sections/\" + strPart + \"/samples.csv\")\n",
    "    pdDfJobsSamples = pd.read_csv(strJobsSamplesFile)\n",
    "    oMinMaxScaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    oMinMaxScaler.fit(pdDfJobsSamples.values)\n",
    "    for name in os.listdir(strJobsDir):\n",
    "        strJobDir = os.path.join(strJobsDir, name)\n",
    "        strSectionSamplesFile = os.path.join(strJobDir, \"samples/sections/\" + strPart + \"/samples.csv\")\n",
    "        pdDfSectionSamples = pd.read_csv(strSectionSamplesFile)\n",
    "        npNArrPreprocessedSectionSamples = oMinMaxScaler.transform(pdDfSectionSamples.values)\n",
    "        pdDfPreprocessedSectionSamples = pd.DataFrame(npNArrPreprocessedSectionSamples, columns=pdDfSectionSamples.columns)\n",
    "        strPreprocessedSectionSamplesDir = os.path.join(strJobDir, \"samples/sections/\" + strPart + \"/preprocessed\")\n",
    "        if os.path.exists(strPreprocessedSectionSamplesDir):\n",
    "            shutil.rmtree(strPreprocessedSectionSamplesDir)\n",
    "        os.mkdir(strPreprocessedSectionSamplesDir)\n",
    "        strPreprocessedSectionSamplesFile = os.path.join(strPreprocessedSectionSamplesDir, \"samples.csv\")\n",
    "        pdDfPreprocessedSectionSamples.to_csv(strPreprocessedSectionSamplesFile, index = False)\n",
    "    return oMinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate training dataset\n",
    "Preprocess data before training instead on the fly. So there is no preprocess callable object in the list of input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_getTrainingDataset(strPart, strJobsDir, \n",
    "                          nReaders = 5, nReadThreads = 5, nParseTreads = 5, nShuffleBufferSize = 1000, nBatchSize = 32):\n",
    "    strSampleFiles = fn_getTrainingSampleFiles(strPart, strJobsDir)\n",
    "    oDataset = tf.data.Dataset.list_files(\"jobs/*/samples/sections/\" + strPart + \"/preprocessed/samples.csv\")\n",
    "    oDataset = oDataset.interleave(lambda strSampleFile: tf.data.TextLineDataset(strSampleFile), cycle_length=nReaders, \n",
    "                                  num_parallel_calls=nReadThreads)\n",
    "    oDataset = oDataset.shuffle(nShuffleBufferSize)\n",
    "    return oDataset.batch(nBatchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "oMinMaxScalerInput = fn_normalizeJobsSectionSamples(\"input\", \"jobs\", \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "g_nBatchSize = 5\n",
    "\n",
    "def fn_DLoss(tensorTrue, tensorPred):\n",
    "        tensorOnes, tensorReal = tensorTrue[:g_nBatchSize, :], tensorPred[:g_nBatchSize, :]\n",
    "        tensorZeros, tensorFake = tensorTrue[g_nBatchSize:, :], tensorPred[g_nBatchSize:, :]\n",
    "        tensorLossReal = math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = tensorReal, labels = tensorOnes), 1)\n",
    "        tensorLossFake = math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = tensorFake, labels = tensorZeros), 1)\n",
    "        tensorLoss = tensorLossReal + tensorLossFake\n",
    "        return math.reduce_mean(tensorLoss)  \n",
    "def fn_GLoss(tensorTrue, tensorPred):\n",
    "    tensorLoss = math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = tensorPred, labels = tensorTrue), 1)\n",
    "    return math.reduce_mean(tensorLoss)\n",
    "\n",
    "class GANOperator(object):\n",
    "    \"\"\"\n",
    "        specify the number of features to be dealt by gan\n",
    "    \"\"\"\n",
    "    def __init__(self, nLatentDim, nHiddenDimG, nHiddenDimD, nBatchSize, nFeatures):\n",
    "        self.nLatentDim = nLatentDim\n",
    "        self.nHiddenDimG = nHiddenDimG\n",
    "        self.nHiddenDimD = nHiddenDimD\n",
    "        self.nBatchSize = nBatchSize\n",
    "        self.nFeatures = nFeatures\n",
    "        \n",
    "        self.oSeqGenerator = None\n",
    "        self.oSeqDiscriminator = None\n",
    "        self.fn_makeGenerator()\n",
    "        self.fn_makeDiscriminator()\n",
    "        self.fn_makeDiscriminatorModel()\n",
    "        self.fn_makeAdversariaModel()\n",
    "        \n",
    "    def fn_makeGenerator(self):\n",
    "        if self.oSeqGenerator:\n",
    "            return self.oSeqGenerator\n",
    "        self.oSeqGenerator = Sequential()\n",
    "        self.oSeqGenerator.add(Dense(self.nHiddenDimG, activation = tf.keras.activations.tanh))\n",
    "        self.oSeqGenerator.add(Dense(self.nFeatures, activation = tf.keras.activations.tanh))\n",
    "    def fn_makeDiscriminator(self):\n",
    "        if self.oSeqDiscriminator:\n",
    "            return self.oSeqDiscriminator\n",
    "        self.oSeqDiscriminator = Sequential()\n",
    "        self.oSeqDiscriminator.add(Dense(self.nHiddenDimD, activation = tf.keras.activations.tanh))\n",
    "        self.oSeqDiscriminator.add(Dense(1, activation = tf.keras.activations.sigmoid))\n",
    "    \n",
    "    def fn_makeDiscriminatorModel(self):\n",
    "        self.oSeqDiscriminatorModel = Sequential()\n",
    "        self.oSeqDiscriminatorModel.add(self.oSeqDiscriminator)\n",
    "        oOptimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "        self.oSeqDiscriminatorModel.compile(loss=fn_DLoss, optimizer=oOptimizer, metrics=[\"accuracy\"])\n",
    "    def fn_makeAdversariaModel(self):\n",
    "        self.oSeqAdversarialModel = Sequential()\n",
    "        self.oSeqAdversarialModel.add(self.oSeqGenerator)\n",
    "        self.oSeqAdversarialModel.add(self.oSeqDiscriminator)\n",
    "        self.oSeqDiscriminator.trainable = False\n",
    "        oOptimer = tf.optimizers.Adam()\n",
    "        self.oSeqAdversarialModel.compile(loss = fn_GLoss, optimizer=oOptimer, metrics=[\"accuracy\"])\n",
    "    \n",
    "    def fn_train(self, npNArrSamples, nEpochs = 2000):\n",
    "        for epoch in range(nEpochs):\n",
    "            npNArrTrueSamples = npNArrSamples[np.random.randint(0, npNArrSamples.shape[0], size = self.nBatchSize), :]\n",
    "            npNoise = np.random.uniform(-1.0, 1.0, size = [self.nBatchSize, self.nLatentDim])\n",
    "            npNArrFake = self.oSeqGenerator.predict(npNoise)\n",
    "            npNArrX = np.concatenate((npNArrTrueSamples, npNArrFake))\n",
    "            npNArrY = np.ones([2 * self.nBatchSize, 1])\n",
    "            npNArrY[self.nBatchSize:, :] = 0\n",
    "            fLossD = self.oSeqDiscriminatorModel.train_on_batch(npNArrX, npNArrY)\n",
    "            \n",
    "            npNArrY = np.ones([self.nBatchSize, 1])\n",
    "            npNoise = np.random.uniform(-1.0, 1.0, size = [self.nBatchSize, self.nLatentDim])\n",
    "            fLossA = self.oSeqAdversarialModel.train_on_batch(npNoise, npNArrY)\n",
    "\n",
    "            strMsg = \"%d: [D loss: %f, acc: %f]\" % (epoch, fLossD[0], fLossD[1])\n",
    "            strMsg = \"%s [A loss: %f, acc: %f]\" % (strMsg, fLossA[0], fLossA[1])\n",
    "            print(strMsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "oGANOperator = GANOperator(nLatentDim=8, nHiddenDimG=16, nHiddenDimD=16, nBatchSize=g_nBatchSize, \n",
    "                           nFeatures=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 1.452051, acc: 0.400000] [A loss: 0.490367, acc: 0.200000]\n",
      "1: [D loss: 1.434356, acc: 0.500000] [A loss: 0.474354, acc: 0.400000]\n",
      "2: [D loss: 1.459817, acc: 0.400000] [A loss: 0.508515, acc: 0.000000]\n",
      "3: [D loss: 1.454618, acc: 0.500000] [A loss: 0.449057, acc: 0.800000]\n",
      "4: [D loss: 1.487272, acc: 0.300000] [A loss: 0.479009, acc: 0.400000]\n",
      "5: [D loss: 1.391144, acc: 0.800000] [A loss: 0.503868, acc: 0.200000]\n",
      "6: [D loss: 1.450166, acc: 0.500000] [A loss: 0.478879, acc: 0.400000]\n",
      "7: [D loss: 1.487112, acc: 0.400000] [A loss: 0.463928, acc: 0.800000]\n",
      "8: [D loss: 1.436899, acc: 0.500000] [A loss: 0.446789, acc: 0.600000]\n",
      "9: [D loss: 1.470820, acc: 0.300000] [A loss: 0.479095, acc: 0.400000]\n"
     ]
    }
   ],
   "source": [
    "oGANOperator.fn_train(npNArrSamples, nEpochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
