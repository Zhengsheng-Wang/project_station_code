{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入包和设置全局路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, datetime\n",
    "import zipfile\n",
    "import lxml.etree as etree\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, pickle, gzip\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 设置路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "strJobsDir = \"../jobs_nobranch\"\n",
    "strZipsDir = \"../zips\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过输入的任务文件夹返回任务文件夹中的所有解调器文件夹路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_getDemodDirsOfAJob(strJobDir):\n",
    "    liststrDemodDirs = [os.path.join(strJobDir, strName) for strName in os.listdir(strJobDir) if \"Demod\" in strName]\n",
    "    return liststrDemodDirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解调器属性选取（更新）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查所有的解调器状态文件发现，以下属性不是所有的解调器都存在：\n",
    "\n",
    "DEMOD_PHASEROTATION，GLOBAL_DEMOD1STATUS，GLOBAL_TEMPSTATUS，DRU_DISKSPACE1，DRU_USEDSPACE1，DRU_FREESPACE1，DRU_USEDPERCENT1\n",
    "\n",
    "对它们进行忽略。\n",
    "\n",
    "目前发现的在数据集中的值只有一种的属性也全部忽略。以后可能会发现它们有多种值。\n",
    "\n",
    "解调器的状态参数按照参数名的前缀来划分大致有3类——DPU（data packet unit），DEMOD、IFU(中频控制单元)，DRU。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读入任务配置文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入任务计划文件\n",
    "\n",
    "首先清空jobs文件夹。\\\n",
    "WorkSch文件中存放了提前计算出的预计接收开始和结束时间。需利用这个接收开始和结束时间截取出有效时间段。\\\n",
    "可能有多个WorkSch文件，根据它的createdTime选择最新的WorkSch文件。\n",
    "\n",
    "只有清华解调器和融为解调器具有反映输出速率的参数，要先把这两个解调器的zip文件路径存储到链表中去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入设备列表文件\n",
    "任务所用的所有设备都在列表文件里。要根据文件中对设备状态文件的属于的设备的描述提取出用于数据接收传输的设备状态文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解调器预处理描述\n",
    "\n",
    "1. 根据任务的设备列表文件找到解调器的设备ID\n",
    "2. 利用设备ID找到解调器的设备状态文件和控制文件，并保存\n",
    "3. 从任务的任务计划文件中读出接收的开始和结束时间，利用接收时间段截取出有效时间段内的记录\n",
    "4. 将解调器的状态参数按照信号处理流程划分为数个最小模块\n",
    "5. 针对每个最小模块产生训练样本和测试样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入设备状态文件和控制文件\n",
    "\n",
    "1. 清空原有的设备文件夹。\n",
    "2. 在设备列表文件中找到解调器设备ID\n",
    "3. 将相应设备状态文件载入设备文件夹\n",
    "\n",
    "执行完成后，任务文件夹里多出如下文件：\n",
    "* 任务名/\n",
    "    * 解调器名/\n",
    "        * raw/\n",
    "            * status.csv\n",
    "        * control.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从zip文件中解压缩具有设备的状态文件，存入raw\n",
    "def fn_extractFilesFromAZip(strJobDir, strID, strZipFile):\n",
    "    strZipDemodStatusFile = \"Status_\" + strID\n",
    "    with zipfile.ZipFile(strZipFile) as oZipFile:\n",
    "        for strFile in oZipFile.namelist():\n",
    "            # 迭代找到zip文件里的解调器的状态文件和控制参数文件\n",
    "            if \"Status_\" + strID in strFile:\n",
    "                # 状态文件\n",
    "                with oZipFile.open(strFile) as f:\n",
    "                    with open(os.path.join(strJobDir, strID + \"/raw/status.csv\"), \"wb\") as f1:\n",
    "                        f1.write(f.read())\n",
    "            elif \"Control_\" + strID in strFile:\n",
    "                # 控制文件\n",
    "                with oZipFile.open(strFile) as f:\n",
    "                    with open(os.path.join(strJobDir, strID + \"/control.csv\"), \"wb\") as f1:\n",
    "                        f1.write(f.read())\n",
    "\n",
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    # 删除原有的解调器文件夹\n",
    "    for strName in os.listdir(strJobDir):\n",
    "        if \"Demod\" in strName:\n",
    "            strDemodDir = os.path.join(strJobDir, strName)\n",
    "            shutil.rmtree(strDemodDir)\n",
    "    \n",
    "    strZipFile = os.path.join(strZipsDir, strJob + \".zip\")\n",
    "    strDeviceListFile = os.path.join(strJobDir, \"device_list.xml\")\n",
    "    oElementTree = etree.parse(strDeviceListFile)\n",
    "    listDevElement = oElementTree.findall(\"./content/deviceList/Device\")\n",
    "    for oElement in listDevElement:\n",
    "        strID = oElement.find(\"DevID\").text\n",
    "        if \"Demod\" in strID:\n",
    "            # 新建解调器文件raw文件夹\n",
    "            os.makedirs(os.path.join(strJobDir, strID + \"/raw\"))\n",
    "            fn_extractFilesFromAZip(strJobDir, strID, strZipFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 截取出有效接收时间范围内的记录\n",
    "\n",
    "1. 首先清空每个解调器文件夹下的valid文件夹\n",
    "2. 然后从work_sch.xml文件中读入数据接收的有效时间段\n",
    "3. 根据有效时间段截取每个解调器的设备状态文件的有效记录，存入valid文件夹\n",
    "执行完成后，任务文件里多出了如下文件：\n",
    "* 任务名/\n",
    "    * 解调器名/\n",
    "        * valid/\n",
    "            * status.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从指定任务文件夹中读取任务所使用的解调器名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分描述\n",
    "解调译码过程按照信号处理流程包含如下阶段：\n",
    "* 中频输入\n",
    "* 载波同步\n",
    "* 比特同步\n",
    "* 维特比译码（I/Q路）\n",
    "* 帧同步（I/Q路）\n",
    "* 译码和解扰（I/Q路）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新建parts文件夹，存放各部分的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 帧同步锁前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 工具函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得到解调器的正常记录index\n",
    "当一条记录的两个帧同步锁都锁上的时候说明该记录在帧同步锁之前的各个阶段均正常，即帧同步锁之前的各个部分用作正常训练样本。曾经考虑对于分路帧同步的情况，如果分路中某一个同步锁锁上了，则可以认为该记录在维特比译码之前各个阶段均正常，但是在分路的情况下，只有一个分路的帧同步锁上比方说I路，不能说明在分路之前的所有阶段均正确，所以舍弃这种做法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按照 index拼接样本csv\n",
    "为了解决前后两部分的“生成样本”时，相同的参数在两部分的处理工作重复的问题，设计该函数，把两个dataframe按照前一个打他frame的index进行拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中频输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据\n",
    "中频输入特征参数：DEMOD_IFLEVEL（输入电平），DEMOD_EBNOVALUE（信噪比）。\\\n",
    "本来还想选用DEMOD_EBNOVALUEQCHL，但是这个参数在整个数据集中取值均为零，故忽略。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加中频输入部分参数名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成样本\n",
    "利用帧同步锁生成正常训练样本和异常样本。该步进行完成后，文件夹具有如下结构：\n",
    "* parts/\n",
    "    * sync/\n",
    "        * input/\n",
    "            * status.csv\n",
    "            * samples/\n",
    "                * normal/\n",
    "                    * train/\n",
    "                        * samples.csv\n",
    "                * abnormal/\n",
    "                    * samples.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化\n",
    "用正则化器适应训练集，再用正则化器去正则化训练集和测试集中的数值属性。将转化的结果存入preprocessed文件夹。该步执行完后，具有的文件夹结构如下：\n",
    "* input/\n",
    "    * samples/\n",
    "        * normal/\n",
    "            * train/\n",
    "                * samples.csv\n",
    "                * preprocessed/\n",
    "                    * samples.csv\n",
    "            * test/\n",
    "                * samples.csv\n",
    "                * preprocessed/\n",
    "                    * samples.csv\n",
    "        * abnormal/\n",
    "            * samples.csv\n",
    "            * preprocessed/\n",
    "                * samples.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载波同步\n",
    "载波同步特征参数：DEMOD_CARRIEROFFSET（载波偏移）、DEMOD_CARRIERLOCK（载波锁）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据\n",
    "载入载波同步部分到carrier文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "listcarrierFeatures = []\n",
    "listFeatures = listcarrierFeatures.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立空的文件夹carrier，将数据导入\n",
    "for strJob in os.listdir(strJobsDir):\n",
    "    liststrDemodDirs = fn_getDemodDirsOfAJob(os.path.join(strJobsDir, strJob))\n",
    "    for strDemodDir in liststrDemodDirs:\n",
    "        strSectionDir = os.path.join(strDemodDir, \"parts/sync/carrier\")\n",
    "        if os.path.exists(strSectionDir):\n",
    "            shutil.rmtree(strSectionDir)\n",
    "        os.mkdir(strSectionDir)\n",
    "    \n",
    "        # 读入有效解调器设备状态参数\n",
    "        strValidStatusFile = os.path.join(strDemodDir, \"valid/status.csv\")\n",
    "        pdDfValidStatus = pd.read_csv(strValidStatusFile, index_col=\"RECTIME\")\n",
    "        pdDfSectionStatus = pdDfValidStatus[listFeatures]\n",
    "        strSectionStatusFile = os.path.join(strSectionDir, \"status.csv\")\n",
    "        pdDfSectionStatus.to_csv(strSectionStatusFile, index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更改二元属性值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(os.path.join(strJobsDir, strJob))\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        pdDfSectionStatus = pd.read_csv(os.path.join(strDemodDir, \"parts/sync/carrier/status.csv\"), index_col=\"RECTIME\")\n",
    "        # 更改属性值\n",
    "        npIndexes = pdDfSectionStatus[\"DEMOD_CARRIERLOCK\"].values\n",
    "        npResult = np.zeros(npIndexes.size)\n",
    "        npResult[npIndexes == 2] = 1\n",
    "        pdDfSectionStatus[\"DEMOD_CARRIERLOCK\"] = npResult\n",
    "        \n",
    "        strRegularizedSamplesDir = os.path.join(strDemodDir, \"parts/sync/carrier/regularized\")\n",
    "        if os.path.exists(strRegularizedSamplesDir):\n",
    "            shutil.rmtree(strRegularizedSamplesDir)\n",
    "        os.mkdir(strRegularizedSamplesDir)\n",
    "        pdDfSectionStatus.to_csv(os.path.join(strRegularizedSamplesDir, \"status.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        strSectionDir = os.path.join(strDemodDir, \"parts/sync/carrier/regularized\")\n",
    "        strLastSamplesDir = os.path.join(strDemodDir, \"parts/sync/input/samples\")\n",
    "        \n",
    "        # 清空样本文件夹\n",
    "        strSamplesDir = os.path.join(strDemodDir, \"parts/sync/carrier/samples\")        \n",
    "        if os.path.exists(strSamplesDir):\n",
    "            shutil.rmtree(strSamplesDir)\n",
    "        os.mkdir(strSamplesDir)\n",
    "        \n",
    "        # 正常训练样本\n",
    "        os.makedirs(os.path.join(strSamplesDir, \"normal/train\"))\n",
    "       \n",
    "        \n",
    "        # 异常样本\n",
    "        os.mkdir(os.path.join(strSamplesDir, \"abnormal\"))\n",
    "        pdDfAbnormalSamples = fn_concat(os.path.join(strLastSamplesDir, \"abnormal/samples.csv\"), \\\n",
    "               os.path.join(strSectionDir, \"status.csv\"))\n",
    "        pdDfAbnormalSamples.to_csv(os.path.join(strSamplesDir, \"abnormal/samples.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对载波同步部分的所有属性进行正则化，包括二元属性（二元属性正则化后不变）。存入preprocessed文件夹。读入所有载波同步正常训练样本，利用正则化器适应正常训练样本。转化所有样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "listinputNormFeatures = [\"DEMOD_IFLEVEL\", \"DEMOD_EBNOVALUE\"]\n",
    "listcarrierNormFeatures = [\"DEMOD_CARRIEROFFSET\"]\n",
    "listNormFeatures = listinputNormFeatures + listcarrierNormFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listNormalTrainingSamples = []\n",
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        pdDfNormalTrainingSamples = pd.read_csv(os.path.join(strDemodDir, \\\n",
    "                                            \"parts/sync/carrier/samples/normal/train/samples.csv\"),\\\n",
    "                                            index_col=\"RECTIME\")\n",
    "        listNormalTrainingSamples.append(pdDfNormalTrainingSamples[listNormFeatures])\n",
    "pdDfNormalTrainingSamples = pd.concat(listNormalTrainingSamples)\n",
    "\n",
    "oMinMaxScaler = sklearn.preprocessing.MinMaxScaler()\n",
    "oMinMaxScaler.fit(pdDfNormalTrainingSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        # 载波同步部分的样本文件夹\n",
    "        strSamplesDir = os.path.join(strDemodDir, \"parts/sync/carrier/samples\")\n",
    "        \n",
    "        # 正则化正常训练集\n",
    "        # 清空预处理文件夹\n",
    "        strPreprocessedNormalTrainingSamplesDir = os.path.join(strSamplesDir, \"normal/train/preprocessed\")\n",
    "        if os.path.exists(strPreprocessedNormalTrainingSamplesDir):\n",
    "            shutil.rmtree(strPreprocessedNormalTrainingSamplesDir)\n",
    "        os.mkdir(strPreprocessedNormalTrainingSamplesDir)\n",
    "        pdDfNormalTrainingSamples = pd.read_csv(os.path.join(strSamplesDir, \"normal/train/samples.csv\"),\\\n",
    "                                                    index_col=\"RECTIME\")\n",
    "        if not pdDfNormalTrainingSamples.empty:\n",
    "            pdDfNormalTrainingSamples.loc[:, listNormFeatures] = \\\n",
    "                oMinMaxScaler.transform(pdDfNormalTrainingSamples.loc[:, listNormFeatures])\n",
    "        \"\"\"输出到磁盘的时候应该去除index，因为后面的tf在读取训练数据时不能识别字符串数据\"\"\"\n",
    "        pdDfNormalTrainingSamples.to_csv(\\\n",
    "              os.path.join(strPreprocessedNormalTrainingSamplesDir, \"samples.csv\"), \\\n",
    "               index=False)\n",
    "        \n",
    "        # 正则化异常样本集\n",
    "        # 清空预处理文件夹\n",
    "        strPreprocessedAbnormalSamplesDir = os.path.join(strSamplesDir, \"abnormal/preprocessed\")\n",
    "        if os.path.exists(strPreprocessedAbnormalSamplesDir):\n",
    "            shutil.rmtree(strPreprocessedAbnormalSamplesDir)\n",
    "        os.mkdir(strPreprocessedAbnormalSamplesDir)\n",
    "        pdDfAbnormalSamples = pd.read_csv(os.path.join(strSamplesDir, \"abnormal/samples.csv\"), \\\n",
    "                                                    index_col=\"RECTIME\")\n",
    "        if not pdDfAbnormalSamples.empty:\n",
    "            pdDfAbnormalSamples.loc[:, listNormFeatures] = \\\n",
    "                oMinMaxScaler.transform(pdDfAbnormalSamples.loc[:, listNormFeatures])\n",
    "        \"\"\"输出到磁盘的时候保留index\"\"\"\n",
    "        pdDfAbnormalSamples.to_csv(\\\n",
    "            os.path.join(strPreprocessedAbnormalSamplesDir, \"samples.csv\"),\\\n",
    "            index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比特同步\n",
    "比特同步的特征参数：DEMOD_BITRATE（比特率）、DEMOD_BITRATEOFFSET（比特偏移）、DEMOD_BITRATEOFFSET（比特偏移Q通道）、DEMOD_BITLOCK（比特锁）、DEMOD_BITLOCKQCHL（比特锁Q通道）、DEMOD_TOTALBITNUMBER（总比特数）、DEMOD_TOTALBITNUMBERQCHL（总比特数Q通道）\n",
    "\n",
    "由于已经有了和比特率相关的参数，所以就不考虑比特数的参数（\"DEMOD_TOTALBITNUMBER\", \"DEMOD_TOTALBITNUMBERQCHL\"）了，因为意义相同。\n",
    "\n",
    "本来对于比特数这个参数，需要用当前秒的比特数减去上一秒的比特数，来得到当前秒接收的比特数。但是减完之后发现，有一些上报秒被的比特数被减成了零。感觉这个参数存在问题，故舍弃。\n",
    "\n",
    "比特率是一个预设值。比特锁属性0为锁上，1为没锁。所以要把它们regularize一下。总比特数是一个累加值，要注意做差分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据\n",
    "存储比特同步增加的属性，更新总的属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "listbitFeatures = [\"DEMOD_BITRATE\", \"DEMOD_BITRATEQCHL\", \"DEMOD_BITRATEOFFSET\", \"DEMOD_BITRATEOFFSETQCHL\",\\\n",
    "                  \"DEMOD_BITLOCK\", \"DEMOD_BITLOCKQCHL\"]\n",
    "listFeatures = listbitFeatures.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        strBitDir = os.path.join(strDemodDir, \"parts/sync/bit\")\n",
    "        if os.path.exists(strBitDir):\n",
    "            shutil.rmtree(strBitDir)\n",
    "        os.mkdir(strBitDir)\n",
    "        \n",
    "        strValidStatusFile = os.path.join(strDemodDir, \"valid/status.csv\")\n",
    "        pdDfValidStatus = pd.read_csv(strValidStatusFile, index_col=\"RECTIME\")\n",
    "        pdDfBitStatus = pdDfValidStatus[listFeatures]\n",
    "        pdDfBitStatus.to_csv(os.path.join(strBitDir, \"status.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更改二元属性的值\n",
    "按照载波同步的方法更改载波锁的二元属性值\n",
    "\n",
    "比特锁属性0为锁上，1为没锁。要把比特锁属性值为0改为1，其它改为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        strSectionDir = os.path.join(strDemodDir, \"parts/sync/bit\")        \n",
    "        pdDfSectionStatus = pd.read_csv(os.path.join(strSectionDir, \"status.csv\"), index_col=\"RECTIME\")\n",
    "        \n",
    "        # 更改bitlock和bitlcokqchl的值\n",
    "        npIndexes = pdDfSectionStatus[\"DEMOD_BITLOCK\"].values\n",
    "        npResult = np.zeros(npIndexes.size)\n",
    "        npResult[npIndexes == 0] = 1\n",
    "        pdDfSectionStatus[\"DEMOD_BITLOCK\"] = npResult\n",
    "        npIndexes = pdDfSectionStatus[\"DEMOD_BITLOCKQCHL\"].values\n",
    "        npResult = np.zeros(npIndexes.size)\n",
    "        npResult[npIndexes == 0] = 1\n",
    "        pdDfSectionStatus[\"DEMOD_BITLOCKQCHL\"] = npResult\n",
    "        \n",
    "        strRegularizedStatusDir = os.path.join(strSectionDir, \"regularized\")\n",
    "        if os.path.exists(strRegularizedStatusDir):\n",
    "            shutil.rmtree(strRegularizedStatusDir)\n",
    "        os.mkdir(strRegularizedStatusDir)\n",
    "        pdDfSectionStatus.to_csv(os.path.join(strRegularizedStatusDir, \"status.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        strSectionDir = os.path.join(strDemodDir, \"parts/sync/bit/regularized\")\n",
    "        strLastSamplesDir = os.path.join(strDemodDir, \"parts/sync/carrier/samples\")\n",
    "        \n",
    "        # 清空样本文件夹\n",
    "        strSamplesDir = os.path.join(strDemodDir, \"parts/sync/bit/samples\")        \n",
    "        if os.path.exists(strSamplesDir):\n",
    "            shutil.rmtree(strSamplesDir)\n",
    "        os.mkdir(strSamplesDir)\n",
    "        \n",
    "        # 正常训练样本\n",
    "        os.makedirs(os.path.join(strSamplesDir, \"normal/train\"))\n",
    "        pdDfNormalSamples = fn_concat(os.path.join(strLastSamplesDir, \"normal/train/samples.csv\"), \\\n",
    "              os.path.join(strSectionDir, \"status.csv\"))\n",
    "        pdDfNormalSamples.to_csv(os.path.join(strSamplesDir, \"normal/train/samples.csv\"), index_label=\"RECTIME\")\n",
    "        \n",
    "        # 异常样本\n",
    "        os.mkdir(os.path.join(strSamplesDir, \"abnormal\"))\n",
    "        pdDfAbnormalSamples = fn_concat(os.path.join(strLastSamplesDir, \"abnormal/samples.csv\"), \\\n",
    "               os.path.join(strSectionDir, \"status.csv\"))\n",
    "        pdDfAbnormalSamples.to_csv(os.path.join(strSamplesDir, \"abnormal/samples.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于二元变量要单独处理。因为如果训练集里所有的某二元变量的值均为1，那么，当用这个训练集训练过后的scaler去转化其它数据集的时候，其他数据集的1变量会被全部转化为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "listbitNormFeatures = [\"DEMOD_BITRATE\", \"DEMOD_BITRATEQCHL\", \"DEMOD_BITRATEOFFSET\", \"DEMOD_BITRATEOFFSETQCHL\"]\n",
    "listNormFeatures = listinputNormFeatures + listcarrierNormFeatures + listbitNormFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listNormalTrainingSamples = []\n",
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        pdDfNormalTrainingSamples = pd.read_csv(os.path.join(strDemodDir, \\\n",
    "                                            \"parts/sync/bit/samples/normal/train/samples.csv\"),\\\n",
    "                                            index_col=\"RECTIME\")\n",
    "        listNormalTrainingSamples.append(pdDfNormalTrainingSamples[listNormFeatures])\n",
    "        \n",
    "pdDfNormalTrainingSamples = pd.concat(listNormalTrainingSamples)\n",
    "oMinMaxScaler = sklearn.preprocessing.MinMaxScaler()\n",
    "oMinMaxScaler.fit(pdDfNormalTrainingSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        # 比特同步部分的样本文件夹\n",
    "        strSamplesDir = os.path.join(strDemodDir, \"parts/sync/bit/samples\")\n",
    "        \n",
    "        # 正则化正常训练集\n",
    "        # 清空预处理文件夹\n",
    "        strPreprocessedNormalTrainingSamplesDir = os.path.join(strSamplesDir, \"normal/train/preprocessed\")\n",
    "        if os.path.exists(strPreprocessedNormalTrainingSamplesDir):\n",
    "            shutil.rmtree(strPreprocessedNormalTrainingSamplesDir)\n",
    "        os.mkdir(strPreprocessedNormalTrainingSamplesDir)\n",
    "        pdDfNormalTrainingSamples = pd.read_csv(os.path.join(strSamplesDir, \"normal/train/samples.csv\"),\\\n",
    "                                                    index_col=\"RECTIME\")\n",
    "        if not pdDfNormalTrainingSamples.empty:\n",
    "            pdDfNormalTrainingSamples.loc[:, listNormFeatures] = \\\n",
    "                oMinMaxScaler.transform(pdDfNormalTrainingSamples.loc[:, listNormFeatures])\n",
    "        \"\"\"输出到磁盘的时候应该去除index，因为后面的tf在读取训练数据时不能识别字符串数据\"\"\"\n",
    "        pdDfNormalTrainingSamples.to_csv(\\\n",
    "              os.path.join(strPreprocessedNormalTrainingSamplesDir, \"samples.csv\"), \\\n",
    "               index=False)\n",
    "        \n",
    "        # 正则化异常样本集\n",
    "        # 清空预处理文件夹\n",
    "        strPreprocessedAbnormalSamplesDir = os.path.join(strSamplesDir, \"abnormal/preprocessed\")\n",
    "        if os.path.exists(strPreprocessedAbnormalSamplesDir):\n",
    "            shutil.rmtree(strPreprocessedAbnormalSamplesDir)\n",
    "        os.mkdir(strPreprocessedAbnormalSamplesDir)\n",
    "        pdDfAbnormalSamples = pd.read_csv(os.path.join(strSamplesDir, \"abnormal/samples.csv\"), \\\n",
    "                                                    index_col=\"RECTIME\")\n",
    "        if not pdDfAbnormalSamples.empty:\n",
    "            pdDfAbnormalSamples.loc[:, listNormFeatures] = \\\n",
    "                oMinMaxScaler.transform(pdDfAbnormalSamples.loc[:, listNormFeatures])\n",
    "        \"\"\"输出到磁盘的时候保留index\"\"\"\n",
    "        pdDfAbnormalSamples.to_csv(\\\n",
    "            os.path.join(strPreprocessedAbnormalSamplesDir, \"samples.csv\"),\\\n",
    "            index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 维特比译码\n",
    "维特比译码具有两套参数，要让模型自己去考虑合路和分路的情况，所以把维特比译码部分的I路和Q路的参数作为一个整体看待。\n",
    "\n",
    "维特比译码部分包含参数：DEMOD_VITERBIINPUT（维特比分路合路开关）、DEMOD_VITERBI1DECODER、DEMOD_VITERBI1LOCK。\n",
    "\n",
    "DEMOD_VITERBIINPUT表示维特比合路或者分路的开关，它具有{0，1，2}三个可能的取值，需要进行“独热编码”。DEMOD_VITERBI1DECODER和DEMOD_VITERBI2DECODER是I路和Q路维特比译码的开关，因为它们已经为二元属性0、1，所以也不用更改它的二元属性值。\n",
    "\n",
    "\n",
    "把独热编码之后的状态参数文件存入diff文件夹，执行完成后具有如下文件结构：\n",
    "* parts/\n",
    "    * sync/\n",
    "        * vi/\n",
    "            * status.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据\n",
    "清空vi文件夹和其下的I、Q文件夹。读入维特比部分的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "listviFeatures = [\"DEMOD_VITERBIINPUT\", \\\n",
    "                   \"DEMOD_VITERBI1DECODER\",\\\n",
    "                   \"DEMOD_VITERBI2DECODER\",\\\n",
    "                   \"DEMOD_VITERBI1LOCK\", \"DEMOD_VITERBI2LOCK\"]\n",
    "listFeatures = listviFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        strStatusDir = os.path.join(strDemodDir, \"parts/sync/vi\")\n",
    "        if os.path.exists(strStatusDir):\n",
    "            shutil.rmtree(strStatusDir)\n",
    "        os.mkdir(strStatusDir)\n",
    "        \n",
    "        pdDfValidStatus = pd.read_csv(os.path.join(strDemodDir, \"valid/status.csv\"), index_col=\"RECTIME\")\n",
    "        pdDfBitStatus = pdDfValidStatus[listFeatures]\n",
    "        pdDfBitStatus.to_csv(os.path.join(strStatusDir, \"status.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 独热编码\n",
    "独热编码DEMOD_VITERBIINPUT，对应关系如下：\n",
    "* 0---000 \n",
    "* 1---010\n",
    "* 2---001\n",
    "\n",
    "每个编码位被重新命名，分别命名为novi、twovi、onevi\n",
    "\n",
    "该步进行完成后，文件结构如下：\n",
    "* onehot/\n",
    "    * status.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    for strDemodDir in fn_getDemodDirsOfAJob(os.path.join(strJobsDir, strJob)):\n",
    "        strSectionDir = os.path.join(strDemodDir, \"parts/sync/vi\")\n",
    "        strOneHotDir = os.path.join(strSectionDir, \"onehot\")\n",
    "        if os.path.exists(strOneHotDir):\n",
    "            shutil.rmtree(strOneHotDir)\n",
    "        os.mkdir(strOneHotDir)\n",
    "        \n",
    "        pdDfStatus = pd.read_csv(os.path.join(strSectionDir, \"status.csv\"), index_col=\"RECTIME\")\n",
    "        \n",
    "        pdDfStatus[\"novi\"] = (pdDfStatus[\"DEMOD_VITERBIINPUT\"] == 0).astype(\"int32\")\n",
    "        pdDfStatus[\"onevi\"] = (pdDfStatus[\"DEMOD_VITERBIINPUT\"] == 1).astype(\"int32\")        \n",
    "        pdDfStatus[\"twovi\"] = (pdDfStatus[\"DEMOD_VITERBIINPUT\"] == 2).astype(\"int32\")        \n",
    "        \n",
    "        pdDfStatus.drop(\"DEMOD_VITERBIINPUT\", inplace = True, axis=1)\n",
    "        \n",
    "        pdDfStatus.to_csv(os.path.join(strOneHotDir, \"status.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成样本\n",
    "从onehot文件夹里取出样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        strSectionDir = os.path.join(strDemodDir, \"parts/sync/vi/onehot\")\n",
    "        strLastSamplesDir = os.path.join(strDemodDir, \"parts/sync/bit/samples\")\n",
    "        \n",
    "        # 清空样本文件夹\n",
    "        strSamplesDir = os.path.join(strDemodDir, \"parts/sync/vi/samples\")        \n",
    "        if os.path.exists(strSamplesDir):\n",
    "            shutil.rmtree(strSamplesDir)\n",
    "        os.mkdir(strSamplesDir)\n",
    "        \n",
    "        # 正常训练样本\n",
    "        os.makedirs(os.path.join(strSamplesDir, \"normal/train\"))\n",
    "        pdDfNormalSamples = fn_concat(os.path.join(strLastSamplesDir, \"normal/train/samples.csv\"), \\\n",
    "              os.path.join(strSectionDir, \"status.csv\"))\n",
    "        pdDfNormalSamples.to_csv(os.path.join(strSamplesDir, \"normal/train/samples.csv\"), index_label=\"RECTIME\")\n",
    "        \n",
    "        # 异常样本\n",
    "        os.mkdir(os.path.join(strSamplesDir, \"abnormal\"))\n",
    "        pdDfAbnormalSamples = fn_concat(os.path.join(strLastSamplesDir, \"abnormal/samples.csv\"), \\\n",
    "               os.path.join(strSectionDir, \"status.csv\"))\n",
    "        pdDfAbnormalSamples.to_csv(os.path.join(strSamplesDir, \"abnormal/samples.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "listviNormFeatures = []\n",
    "listNormFeatures = listinputNormFeatures + listcarrierNormFeatures + listbitNormFeatures + listviNormFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listNormalTrainingSamples = []\n",
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        pdDfNormalTrainingSamples = pd.read_csv(os.path.join(strDemodDir, \\\n",
    "                                            \"parts/sync/vi/samples/normal/train/samples.csv\"),\\\n",
    "                                            index_col=\"RECTIME\")\n",
    "        listNormalTrainingSamples.append(pdDfNormalTrainingSamples[listNormFeatures])\n",
    "        \n",
    "pdDfNormalTrainingSamples = pd.concat(listNormalTrainingSamples)\n",
    "oMinMaxScaler = sklearn.preprocessing.MinMaxScaler()\n",
    "oMinMaxScaler.fit(pdDfNormalTrainingSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        # 维特比译码的样本文件夹\n",
    "        strSamplesDir = os.path.join(strDemodDir, \"parts/sync/vi/samples\")\n",
    "        \n",
    "        # 正则化正常训练集\n",
    "        # 清空预处理文件夹\n",
    "        strPreprocessedNormalTrainingSamplesDir = os.path.join(strSamplesDir, \"normal/train/preprocessed\")\n",
    "        if os.path.exists(strPreprocessedNormalTrainingSamplesDir):\n",
    "            shutil.rmtree(strPreprocessedNormalTrainingSamplesDir)\n",
    "        os.mkdir(strPreprocessedNormalTrainingSamplesDir)\n",
    "        pdDfNormalTrainingSamples = pd.read_csv(os.path.join(strSamplesDir, \"normal/train/samples.csv\"),\\\n",
    "                                                    index_col=\"RECTIME\")\n",
    "        if not pdDfNormalTrainingSamples.empty:\n",
    "            pdDfNormalTrainingSamples.loc[:, listNormFeatures] = \\\n",
    "                oMinMaxScaler.transform(pdDfNormalTrainingSamples.loc[:, listNormFeatures])\n",
    "        \"\"\"输出到磁盘的时候应该去除index，因为后面的tf在读取训练数据时不能识别字符串数据\"\"\"\n",
    "        pdDfNormalTrainingSamples.to_csv(\\\n",
    "              os.path.join(strPreprocessedNormalTrainingSamplesDir, \"samples.csv\"), \\\n",
    "               index=False)\n",
    "        \n",
    "        # 正则化异常样本集\n",
    "        # 清空预处理文件夹\n",
    "        strPreprocessedAbnormalSamplesDir = os.path.join(strSamplesDir, \"abnormal/preprocessed\")\n",
    "        if os.path.exists(strPreprocessedAbnormalSamplesDir):\n",
    "            shutil.rmtree(strPreprocessedAbnormalSamplesDir)\n",
    "        os.mkdir(strPreprocessedAbnormalSamplesDir)\n",
    "        pdDfAbnormalSamples = pd.read_csv(os.path.join(strSamplesDir, \"abnormal/samples.csv\"), \\\n",
    "                                                    index_col=\"RECTIME\")\n",
    "        if not pdDfAbnormalSamples.empty:\n",
    "            pdDfAbnormalSamples.loc[:, listNormFeatures] = \\\n",
    "                oMinMaxScaler.transform(pdDfAbnormalSamples.loc[:, listNormFeatures])\n",
    "        \"\"\"输出到磁盘的时候保留index\"\"\"\n",
    "        pdDfAbnormalSamples.to_csv(\\\n",
    "            os.path.join(strPreprocessedAbnormalSamplesDir, \"samples.csv\"),\\\n",
    "            index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 帧同步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据\n",
    "在parts/sync下新建frame文件夹。将数据存入frame中。该步结束后，文件结构如下：\n",
    "* parts/\n",
    "    * sync/\n",
    "        * frame/\n",
    "            * status.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "listframeFeatures = [\"DEMOD_FRAMESYNCINPUT\",\\\n",
    "                    \"DPU_FRAMELEN1\", \"DPU_FRAMEHEADLEN1\", \"DPU_RECEIVEDFRAMECOUNTER1\", \\\n",
    "                     \"DPU_DROPOUTFRAMECOUNTER1\", \\\n",
    "                     \"DPU_FRAMELEN2\", \"DPU_FRAMEHEADLEN2\", \"DPU_RECEIVEDFRAMECOUNTER2\", \\\n",
    "                     \"DPU_DROPOUTFRAMECOUNTER2\"]\n",
    "listFeatures = listframeFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        strStatusDir = os.path.join(strDemodDir, \"parts/sync/frame\")\n",
    "        if os.path.exists(strStatusDir):\n",
    "            shutil.rmtree(strStatusDir)\n",
    "        os.mkdir(strStatusDir)\n",
    "        \n",
    "        pdDfValidStatus = pd.read_csv(os.path.join(strDemodDir, \"valid/status.csv\"), index_col=\"RECTIME\")\n",
    "        pdDfValidStatus[listFeatures].to_csv(os.path.join(strStatusDir, \"status.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算每秒的增加量\n",
    "该步完成后，文件结构如下：\n",
    "* frame/\n",
    "    * status.csv\n",
    "    * diff/\n",
    "        * status.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "listframeAggreFeatures = [\"DPU_RECEIVEDFRAMECOUNTER1\", \"DPU_DROPOUTFRAMECOUNTER1\", \\\n",
    "                          \"DPU_RECEIVEDFRAMECOUNTER2\", \"DPU_DROPOUTFRAMECOUNTER2\"]\n",
    "listAggreFeatures = listframeAggreFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_subNumber(nWin):\n",
    "        if nWin.size == 1:\n",
    "            return  nWin[0]\n",
    "        elif nWin[1] < nWin[0]:\n",
    "                return nWin[1]\n",
    "        else:\n",
    "                return nWin[1] - nWin[0]\n",
    "\n",
    "# 把0元素全部改为它的上一个元素的值\n",
    "def fn_fillZeros(nWin):\n",
    "    if nWin.size == 1:\n",
    "        return nWin[0]\n",
    "    elif nWin[1] == 0:\n",
    "        return nWin[0]\n",
    "    else:\n",
    "        return nWin[1]\n",
    "\n",
    "# 对比特数属性存入diff\n",
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        # 读入原始状态参数\n",
    "        pdDfRawStatus = pd.read_csv(os.path.join(strDemodDir, \"raw/status.csv\"), index_col=\"RECTIME\")\n",
    "        strStatusDir = os.path.join(strDemodDir, \"parts/sync/frame\")\n",
    "        # 清空diff\n",
    "        strDiffDir = os.path.join(strStatusDir, \"diff\")\n",
    "        if os.path.exists(strDiffDir):\n",
    "            shutil.rmtree(strDiffDir)\n",
    "        os.mkdir(strDiffDir)\n",
    "        pdDfStatus = pd.read_csv(os.path.join(strStatusDir, \"status.csv\"), index_col=\"RECTIME\")\n",
    "        # 从第二个有效上报时间点开始，算每个点单独的包数\n",
    "        pdDfStatus[listAggreFeatures] = pdDfStatus[listAggreFeatures].rolling(window=2, min_periods=1).\\\n",
    "        apply(fn_subNumber, raw=True)\n",
    "        # 找到第一个有效时间点之前的时间点的整数坐标\n",
    "        strFirstValidIndex = pdDfStatus.index[0]\n",
    "        i = 0\n",
    "        for strIndex in pdDfRawStatus.index:\n",
    "            if strIndex == strFirstValidIndex:\n",
    "                # 取出第一个有效时间点之前的记录和第一个有效时间点记录\n",
    "                pdSeriesRaw = pdDfRawStatus.loc[pdDfRawStatus.index[i - 1], listAggreFeatures]\n",
    "                pdSeriesToBeCulled = pdDfStatus.loc[strFirstValidIndex, listAggreFeatures]\n",
    "                for j in range(len(pdSeriesToBeCulled)):\n",
    "                    # 如果第一个有效点记录的在某域上的值大于等于相应的第一个有效时间点之前的记录的值，则做差\n",
    "                    pdSeriesToBeCulled[j] = pdSeriesToBeCulled[j] - pdSeriesRaw[j] \\\n",
    "                        if pdSeriesToBeCulled[j] >= pdSeriesRaw[j] else pdSeriesToBeCulled[j]\n",
    "                pdDfStatus.loc[strFirstValidIndex, listAggreFeatures] = pdSeriesToBeCulled\n",
    "            i += 1\n",
    "        # 填充0值\n",
    "        pdDfStatus[listAggreFeatures] = pdDfStatus[listAggreFeatures].rolling(window=2, min_periods=1).\\\n",
    "        apply(fn_fillZeros, raw=True)\n",
    "        pdDfStatus.to_csv(os.path.join(strDiffDir, \"status.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 独热编码\n",
    "DEMOD_FRAMESYNCINPUT需要独热编码，对应关系如下：\n",
    "* 0---100 noframe\n",
    "* 1---010 oneframe\n",
    "* 2---001 twoframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    for strDemodDir in fn_getDemodDirsOfAJob(os.path.join(strJobsDir, strJob)):\n",
    "        strSectionDir = os.path.join(strDemodDir, \"parts/sync/frame\")\n",
    "        strOneHotDir = os.path.join(strSectionDir, \"onehot\")\n",
    "        if os.path.exists(strOneHotDir):\n",
    "            shutil.rmtree(strOneHotDir)\n",
    "        os.mkdir(strOneHotDir)\n",
    "        \n",
    "        pdDfStatus = pd.read_csv(os.path.join(strSectionDir, \"diff/status.csv\"), index_col=\"RECTIME\")\n",
    "        \n",
    "        pdDfStatus[\"noframe\"] = (pdDfStatus[\"DEMOD_FRAMESYNCINPUT\"] == 0).astype(\"int32\")\n",
    "        pdDfStatus[\"oneframe\"] = (pdDfStatus[\"DEMOD_FRAMESYNCINPUT\"] == 1).astype(\"int32\")        \n",
    "        pdDfStatus[\"twoframe\"] = (pdDfStatus[\"DEMOD_FRAMESYNCINPUT\"] == 2).astype(\"int32\")        \n",
    "        \n",
    "        pdDfStatus.drop(\"DEMOD_FRAMESYNCINPUT\", inplace = True, axis=1)\n",
    "        \n",
    "        pdDfStatus.to_csv(os.path.join(strOneHotDir, \"status.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        strSectionDir = os.path.join(strDemodDir, \"parts/sync/frame/onehot\")\n",
    "        strLastSamplesDir = os.path.join(strDemodDir, \"parts/sync/vi/samples\")\n",
    "        \n",
    "        # 清空样本文件夹\n",
    "        strSamplesDir = os.path.join(strDemodDir, \"parts/sync/frame/samples\")        \n",
    "        if os.path.exists(strSamplesDir):\n",
    "            shutil.rmtree(strSamplesDir)\n",
    "        os.mkdir(strSamplesDir)\n",
    "        \n",
    "        # 正常训练样本\n",
    "        os.makedirs(os.path.join(strSamplesDir, \"normal/train\"))\n",
    "        pdDfNormalSamples = fn_concat(os.path.join(strLastSamplesDir, \"normal/train/samples.csv\"), \\\n",
    "              os.path.join(strSectionDir, \"status.csv\"))\n",
    "        pdDfNormalSamples.to_csv(os.path.join(strSamplesDir, \"normal/train/samples.csv\"), index_label=\"RECTIME\")\n",
    "        \n",
    "        # 异常样本\n",
    "        os.mkdir(os.path.join(strSamplesDir, \"abnormal\"))\n",
    "        pdDfAbnormalSamples = fn_concat(os.path.join(strLastSamplesDir, \"abnormal/samples.csv\"), \\\n",
    "               os.path.join(strSectionDir, \"status.csv\"))\n",
    "        pdDfAbnormalSamples.to_csv(os.path.join(strSamplesDir, \"abnormal/samples.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "listframeNormFeatures = [\"DPU_FRAMELEN1\", \"DPU_FRAMEHEADLEN1\", \"DPU_RECEIVEDFRAMECOUNTER1\", \\\n",
    "                         \"DPU_DROPOUTFRAMECOUNTER1\", \\\n",
    "                         \"DPU_FRAMELEN2\", \"DPU_FRAMEHEADLEN2\", \"DPU_RECEIVEDFRAMECOUNTER2\", \\\n",
    "                         \"DPU_DROPOUTFRAMECOUNTER2\"]\n",
    "listNormFeatures = listinputNormFeatures + listcarrierNormFeatures + listbitNormFeatures + listviNormFeatures + \\\n",
    "    listframeNormFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listNormalTrainingSamples = []\n",
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        pdDfNormalTrainingSamples = pd.read_csv(os.path.join(strDemodDir, \\\n",
    "                                            \"parts/sync/frame/samples/normal/train/samples.csv\"),\\\n",
    "                                            index_col=\"RECTIME\")\n",
    "        listNormalTrainingSamples.append(pdDfNormalTrainingSamples[listNormFeatures])\n",
    "        \n",
    "pdDfNormalTrainingSamples = pd.concat(listNormalTrainingSamples)\n",
    "oMinMaxScaler = sklearn.preprocessing.MinMaxScaler()\n",
    "oMinMaxScaler.fit(pdDfNormalTrainingSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        # 维特比译码的样本文件夹\n",
    "        strSamplesDir = os.path.join(strDemodDir, \"parts/sync/frame/samples\")\n",
    "        \n",
    "        # 正则化正常训练集\n",
    "        # 清空预处理文件夹\n",
    "        strPreprocessedNormalTrainingSamplesDir = os.path.join(strSamplesDir, \"normal/train/preprocessed\")\n",
    "        if os.path.exists(strPreprocessedNormalTrainingSamplesDir):\n",
    "            shutil.rmtree(strPreprocessedNormalTrainingSamplesDir)\n",
    "        os.mkdir(strPreprocessedNormalTrainingSamplesDir)\n",
    "        pdDfNormalTrainingSamples = pd.read_csv(os.path.join(strSamplesDir, \"normal/train/samples.csv\"),\\\n",
    "                                                    index_col=\"RECTIME\")\n",
    "        if not pdDfNormalTrainingSamples.empty:\n",
    "            pdDfNormalTrainingSamples.loc[:, listNormFeatures] = \\\n",
    "                oMinMaxScaler.transform(pdDfNormalTrainingSamples.loc[:, listNormFeatures])\n",
    "        \"\"\"输出到磁盘的时候应该去除index，因为后面的tf在读取训练数据时不能识别字符串数据\"\"\"\n",
    "        pdDfNormalTrainingSamples.to_csv(\\\n",
    "              os.path.join(strPreprocessedNormalTrainingSamplesDir, \"samples.csv\"), \\\n",
    "               index=False)\n",
    "        \n",
    "        # 正则化异常样本集\n",
    "        # 清空预处理文件夹\n",
    "        strPreprocessedAbnormalSamplesDir = os.path.join(strSamplesDir, \"abnormal/preprocessed\")\n",
    "        if os.path.exists(strPreprocessedAbnormalSamplesDir):\n",
    "            shutil.rmtree(strPreprocessedAbnormalSamplesDir)\n",
    "        os.mkdir(strPreprocessedAbnormalSamplesDir)\n",
    "        pdDfAbnormalSamples = pd.read_csv(os.path.join(strSamplesDir, \"abnormal/samples.csv\"), \\\n",
    "                                                    index_col=\"RECTIME\")\n",
    "        if not pdDfAbnormalSamples.empty:\n",
    "            pdDfAbnormalSamples.loc[:, listNormFeatures] = \\\n",
    "                oMinMaxScaler.transform(pdDfAbnormalSamples.loc[:, listNormFeatures])\n",
    "        \"\"\"输出到磁盘的时候保留index\"\"\"\n",
    "        pdDfAbnormalSamples.to_csv(\\\n",
    "            os.path.join(strPreprocessedAbnormalSamplesDir, \"samples.csv\"),\\\n",
    "            index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造不做帧同步的数据\n",
    "执行完成后文件结构如下：\n",
    "* frame/\n",
    "    * samples/\n",
    "        * normal/\n",
    "            * train/\n",
    "                * preprocessed/\n",
    "                    * samples.csv\n",
    "                    * samples_noframe.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"需要置零的参数名\"\"\"\n",
    "listSettingFeatures = [\"DPU_RECEIVEDFRAMECOUNTER1\", \"DPU_DROPOUTFRAMECOUNTER1\", \\\n",
    "                      \"DPU_RECEIVEDFRAMECOUNTER2\", \"DPU_DROPOUTFRAMECOUNTER2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    for strDemodDir in fn_getDemodDirsOfAJob(os.path.join(strJobsDir, strJob)):\n",
    "        strSamplesDir = os.path.join(strDemodDir, \"parts/sync/frame/samples/normal/train/preprocessed\")\n",
    "        \n",
    "        pdDfSamples = pd.read_csv(os.path.join(strSamplesDir, \"samples.csv\"))\n",
    "        pdDfSamples.loc[:, listSettingFeatures] = 0.\n",
    "        \n",
    "        pdDfSamples.to_csv(os.path.join(strSamplesDir, \"samples_noframe.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 帧同步后——译码和解扰部分\n",
    "DPU_DESCRAMBLING1、DPU_DESCRAMBLINGPOLYNOMIAL1、DPU_DESCRAMBLINGPRESETVALUE1、DPU_DECODEPOS1、DPU_LDPCSTATUS1、DPU_RSSTATUS1、DPU_RSGOODFRAMECOUNTER1（rs和ldpc共用）、DPU_RSBADFRAMECOUNTER1（rs和ldpc共用）、DPU_RSERRORBITNUMBER1、DPU_CRCSTATUS1、DPU_CRCGOODFRAMECOUNTER1、DPU_CRCBADFRAMECOUNTER1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "listdecodeFeatures = [\"DPU_DESCRAMBLING1\", \"DPU_DESCRAMBLINGPOLYNOMIAL1\", \"DPU_DESCRAMBLINGPRESETVALUE1\",\\\n",
    "                     \"DPU_DESCRAMBLING2\", \"DPU_DESCRAMBLINGPOLYNOMIAL2\", \"DPU_DESCRAMBLINGPRESETVALUE2\",\\\n",
    "                      \n",
    "                     \"DPU_DECODEPOS1\", \"DPU_LDPCSTATUS1\", \"DPU_RSSTATUS1\", \"DPU_RSGOODFRAMECOUNTER1\",\\\n",
    "                      \"DPU_RSBADFRAMECOUNTER1\", \"DPU_RSERRORBITNUMBER1\",\\\n",
    "                     \"DPU_DECODEPOS2\", \"DPU_LDPCSTATUS2\", \"DPU_RSSTATUS2\", \"DPU_RSGOODFRAMECOUNTER2\",\\\n",
    "                      \"DPU_RSBADFRAMECOUNTER2\", \"DPU_RSERRORBITNUMBER2\",\\\n",
    "                      \n",
    "                     \"DPU_CRCSTATUS1\", \"DPU_CRCGOODFRAMECOUNTER1\", \"DPU_CRCBADFRAMECOUNTER1\",\\\n",
    "                     \"DPU_CRCSTATUS2\", \"DPU_CRCGOODFRAMECOUNTER2\", \"DPU_CRCBADFRAMECOUNTER2\"]\n",
    "listFeatures = listdecodeFeatures.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        strStatusDir = os.path.join(strDemodDir, \"parts/decode\")\n",
    "        if os.path.exists(strStatusDir):\n",
    "            shutil.rmtree(strStatusDir)\n",
    "        os.mkdir(strStatusDir)\n",
    "        \n",
    "        pdDfValidStatus = pd.read_csv(os.path.join(strDemodDir, \"valid/status.csv\"), index_col=\"RECTIME\")\n",
    "        pdDfValidStatus[listFeatures].to_csv(os.path.join(strStatusDir, \"status.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算每秒的增加量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "listdecodeAggreFeatures = [\"DPU_RSGOODFRAMECOUNTER1\", \"DPU_RSBADFRAMECOUNTER1\", \"DPU_RSERRORBITNUMBER1\",\\\n",
    "                          \"DPU_CRCGOODFRAMECOUNTER1\", \"DPU_CRCBADFRAMECOUNTER1\",\\\n",
    "                          \"DPU_RSGOODFRAMECOUNTER2\", \"DPU_RSBADFRAMECOUNTER2\", \"DPU_RSERRORBITNUMBER2\",\\\n",
    "                          \"DPU_CRCGOODFRAMECOUNTER2\", \"DPU_CRCBADFRAMECOUNTER2\"]\n",
    "listAggreFeatures = listdecodeAggreFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比特数属性存入diff\n",
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        # 读入原始状态参数\n",
    "        pdDfRawStatus = pd.read_csv(os.path.join(strDemodDir, \"raw/status.csv\"), index_col=\"RECTIME\")\n",
    "        strStatusDir = os.path.join(strDemodDir, \"parts/decode\")\n",
    "        # 清空diff\n",
    "        strDiffDir = os.path.join(strStatusDir, \"diff\")\n",
    "        if os.path.exists(strDiffDir):\n",
    "            shutil.rmtree(strDiffDir)\n",
    "        os.mkdir(strDiffDir)\n",
    "        pdDfStatus = pd.read_csv(os.path.join(strStatusDir, \"status.csv\"), index_col=\"RECTIME\")\n",
    "        # 从第二个有效上报时间点开始，算每个点单独的包数\n",
    "        pdDfStatus[listAggreFeatures] = pdDfStatus[listAggreFeatures].rolling(window=2, min_periods=1).\\\n",
    "        apply(fn_subNumber, raw=True)\n",
    "        # 找到第一个有效时间点之前的时间点的整数坐标\n",
    "        strFirstValidIndex = pdDfStatus.index[0]\n",
    "        i = 0\n",
    "        for strIndex in pdDfRawStatus.index:\n",
    "            if strIndex == strFirstValidIndex:\n",
    "                # 取出第一个有效时间点之前的记录和第一个有效时间点记录\n",
    "                pdSeriesRaw = pdDfRawStatus.loc[pdDfRawStatus.index[i - 1], listAggreFeatures]\n",
    "                pdSeriesToBeCulled = pdDfStatus.loc[strFirstValidIndex, listAggreFeatures]\n",
    "                for j in range(len(pdSeriesToBeCulled)):\n",
    "                    # 如果第一个有效点记录的在某域上的值大于等于相应的第一个有效时间点之前的记录的值，则做差\n",
    "                    pdSeriesToBeCulled[j] = pdSeriesToBeCulled[j] - pdSeriesRaw[j] \\\n",
    "                        if pdSeriesToBeCulled[j] >= pdSeriesRaw[j] else pdSeriesToBeCulled[j]\n",
    "                pdDfStatus.loc[strFirstValidIndex, listAggreFeatures] = pdSeriesToBeCulled\n",
    "            i += 1\n",
    "        # 填充0值\n",
    "        pdDfStatus[listAggreFeatures] = pdDfStatus[listAggreFeatures].rolling(window=2, min_periods=1).\\\n",
    "        apply(fn_fillZeros, raw=True)\n",
    "        pdDfStatus.to_csv(os.path.join(strDiffDir, \"status.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 独热编码\n",
    "DPU_DESCRAMBLING需要独热编码，它具有的值和对应关系如下：\n",
    "{0, 1, 2, 3}\n",
    "* 0---1000 descr0\n",
    "* 1---0100 descr1\n",
    "* 2---0010 descr2\n",
    "* 3---0001 descr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    for strDemodDir in fn_getDemodDirsOfAJob(os.path.join(strJobsDir, strJob)):\n",
    "        strSectionDir = os.path.join(strDemodDir, \"parts/decode\")\n",
    "        strOneHotDir = os.path.join(strSectionDir, \"onehot\")\n",
    "        if os.path.exists(strOneHotDir):\n",
    "            shutil.rmtree(strOneHotDir)\n",
    "        os.mkdir(strOneHotDir)\n",
    "        \n",
    "        pdDfStatus = pd.read_csv(os.path.join(strSectionDir, \"diff/status.csv\"), index_col=\"RECTIME\")\n",
    "        \n",
    "        pdDfStatus[\"descr10\"] = (pdDfStatus[\"DPU_DESCRAMBLING1\"] == 0).astype(\"int32\")\n",
    "        pdDfStatus[\"descr11\"] = (pdDfStatus[\"DPU_DESCRAMBLING1\"] == 1).astype(\"int32\")        \n",
    "        pdDfStatus[\"descr12\"] = (pdDfStatus[\"DPU_DESCRAMBLING1\"] == 2).astype(\"int32\")  \n",
    "        pdDfStatus[\"descr13\"] = (pdDfStatus[\"DPU_DESCRAMBLING1\"] == 3).astype(\"int32\")  \n",
    "\n",
    "        pdDfStatus[\"descr20\"] = (pdDfStatus[\"DPU_DESCRAMBLING2\"] == 0).astype(\"int32\")\n",
    "        pdDfStatus[\"descr21\"] = (pdDfStatus[\"DPU_DESCRAMBLING2\"] == 1).astype(\"int32\")        \n",
    "        pdDfStatus[\"descr22\"] = (pdDfStatus[\"DPU_DESCRAMBLING2\"] == 2).astype(\"int32\")     \n",
    "        pdDfStatus[\"descr23\"] = (pdDfStatus[\"DPU_DESCRAMBLING2\"] == 3).astype(\"int32\")     \n",
    "\n",
    "        pdDfStatus.drop([\"DPU_DESCRAMBLING1\", \"DPU_DESCRAMBLING2\"], inplace = True, axis=1)\n",
    "        \n",
    "        pdDfStatus.to_csv(os.path.join(strOneHotDir, \"status.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        strSectionDir = os.path.join(strDemodDir, \"parts/decode/onehot\")\n",
    "        strLastSamplesDir = os.path.join(strDemodDir, \"parts/sync/input/samples\")\n",
    "        \n",
    "        # 清空样本文件夹\n",
    "        strSamplesDir = os.path.join(strDemodDir, \"parts/decode/samples\")        \n",
    "        if os.path.exists(strSamplesDir):\n",
    "            shutil.rmtree(strSamplesDir)\n",
    "        os.mkdir(strSamplesDir)\n",
    "        \n",
    "        # 正常训练样本\n",
    "        os.makedirs(os.path.join(strSamplesDir, \"normal/train\"))\n",
    "        pdDfNormalSamples = fn_concat(os.path.join(strLastSamplesDir, \"normal/train/samples.csv\"), \\\n",
    "              os.path.join(strSectionDir, \"status.csv\"))\n",
    "        pdDfNormalSamples.to_csv(os.path.join(strSamplesDir, \"normal/train/samples.csv\"), index_label=\"RECTIME\")\n",
    "        \n",
    "        # 异常样本\n",
    "        os.mkdir(os.path.join(strSamplesDir, \"abnormal\"))\n",
    "        pdDfAbnormalSamples = fn_concat(os.path.join(strLastSamplesDir, \"abnormal/samples.csv\"), \\\n",
    "               os.path.join(strSectionDir, \"status.csv\"))\n",
    "        pdDfAbnormalSamples.to_csv(os.path.join(strSamplesDir, \"abnormal/samples.csv\"), index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "listdecodeNormFeatures = [\"DPU_DESCRAMBLINGPOLYNOMIAL1\", \"DPU_DESCRAMBLINGPRESETVALUE1\",\\\n",
    "                         \"DPU_DESCRAMBLINGPOLYNOMIAL2\", \"DPU_DESCRAMBLINGPRESETVALUE2\",\\\n",
    "                          \n",
    "                         \"DPU_RSGOODFRAMECOUNTER1\", \"DPU_RSBADFRAMECOUNTER1\", \"DPU_RSERRORBITNUMBER1\",\\\n",
    "                         \"DPU_RSGOODFRAMECOUNTER2\", \"DPU_RSBADFRAMECOUNTER2\", \"DPU_RSERRORBITNUMBER2\",\\\n",
    "                         \n",
    "                         \"DPU_CRCGOODFRAMECOUNTER1\", \"DPU_CRCBADFRAMECOUNTER1\",\\\n",
    "                         \"DPU_CRCGOODFRAMECOUNTER2\", \"DPU_CRCBADFRAMECOUNTER2\"]\n",
    "listNormFeatures = listinputNormFeatures + listdecodeNormFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listNormalTrainingSamples = []\n",
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        pdDfNormalTrainingSamples = pd.read_csv(os.path.join(strDemodDir, \\\n",
    "                                            \"parts/decode/samples/normal/train/samples.csv\"),\\\n",
    "                                            index_col=\"RECTIME\")\n",
    "        listNormalTrainingSamples.append(pdDfNormalTrainingSamples[listNormFeatures])\n",
    "        \n",
    "pdDfNormalTrainingSamples = pd.concat(listNormalTrainingSamples)\n",
    "oMinMaxScaler = sklearn.preprocessing.MinMaxScaler()\n",
    "oMinMaxScaler.fit(pdDfNormalTrainingSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strJob in os.listdir(strJobsDir):\n",
    "    strJobDir = os.path.join(strJobsDir, strJob)\n",
    "    listDemodDirs = fn_getDemodDirsOfAJob(strJobDir)\n",
    "    for strDemodDir in listDemodDirs:\n",
    "        strSamplesDir = os.path.join(strDemodDir, \"parts/decode/samples\")\n",
    "        \n",
    "        # 正则化正常训练集\n",
    "        # 清空预处理文件夹\n",
    "        strPreprocessedNormalTrainingSamplesDir = os.path.join(strSamplesDir, \"normal/train/preprocessed\")\n",
    "        if os.path.exists(strPreprocessedNormalTrainingSamplesDir):\n",
    "            shutil.rmtree(strPreprocessedNormalTrainingSamplesDir)\n",
    "        os.mkdir(strPreprocessedNormalTrainingSamplesDir)\n",
    "        pdDfNormalTrainingSamples = pd.read_csv(os.path.join(strSamplesDir, \"normal/train/samples.csv\"),\\\n",
    "                                                    index_col=\"RECTIME\")\n",
    "        if not pdDfNormalTrainingSamples.empty:\n",
    "            pdDfNormalTrainingSamples.loc[:, listNormFeatures] = \\\n",
    "                oMinMaxScaler.transform(pdDfNormalTrainingSamples.loc[:, listNormFeatures])\n",
    "        \"\"\"输出到磁盘的时候应该去除index，因为后面的tf在读取训练数据时不能识别字符串数据\"\"\"\n",
    "        pdDfNormalTrainingSamples.to_csv(\\\n",
    "              os.path.join(strPreprocessedNormalTrainingSamplesDir, \"samples.csv\"), \\\n",
    "               index=False)\n",
    "        \n",
    "        # 正则化异常样本集\n",
    "        # 清空预处理文件夹\n",
    "        strPreprocessedAbnormalSamplesDir = os.path.join(strSamplesDir, \"abnormal/preprocessed\")\n",
    "        if os.path.exists(strPreprocessedAbnormalSamplesDir):\n",
    "            shutil.rmtree(strPreprocessedAbnormalSamplesDir)\n",
    "        os.mkdir(strPreprocessedAbnormalSamplesDir)\n",
    "        pdDfAbnormalSamples = pd.read_csv(os.path.join(strSamplesDir, \"abnormal/samples.csv\"), \\\n",
    "                                                    index_col=\"RECTIME\")\n",
    "        if not pdDfAbnormalSamples.empty:\n",
    "            pdDfAbnormalSamples.loc[:, listNormFeatures] = \\\n",
    "                oMinMaxScaler.transform(pdDfAbnormalSamples.loc[:, listNormFeatures])\n",
    "        \"\"\"输出到磁盘的时候保留index\"\"\"\n",
    "        pdDfAbnormalSamples.to_csv(\\\n",
    "            os.path.join(strPreprocessedAbnormalSamplesDir, \"samples.csv\"),\\\n",
    "            index_label=\"RECTIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习一下pdnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"读入投票csv\"\"\"\n",
    "pdDfRatings = pd.read_csv(\"../ml-20m/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDfRatings.userId = pdDfRatings.userId.astype(\"int\")\n",
    "pdDfRatings.movieId = pdDfRatings.movieId.astype(\"int\")\n",
    "pdDfRatings.rating = pdDfRatings.rating.astype(\"float\")\n",
    "pdDfRatings.timestamp = pdDfRatings.timestamp.apply(\\\n",
    "                            lambda s: datetime.datetime.utcfromtimestamp(s).strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdIndexMovie = pdDfRatings.groupby(\"movieId\").count().sort_values(\"rating\", ascending=False)[:1000].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDfRatings2 = pdDfRatings.loc[pdDfRatings.movieId.isin(pdIndexMovie), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdIndexUser = pdDfRatings2.groupby(\"userId\").count().sort_values(\"rating\").sample(n=1000, random_state=2020).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDfRatings3 =pdDfRatings2.loc[pdDfRatings2.userId.isin(pdIndexUser), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDfMovieIdMap = pd.DataFrame(data=pdDfRatings3.movieId.unique(), columns=[\"oldMovieId\"])\n",
    "pdDfMovieIdMap[\"newMovieId\"] = pdDfMovieIdMap.index + 1\n",
    "\n",
    "pdDfUserIdMap = pd.DataFrame(data=pdDfRatings3.userId.unique(), columns=[\"oldUserId\"])\n",
    "pdDfUserIdMap[\"newUserId\"] = pdDfUserIdMap.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDfRatings3 = pdDfRatings3.merge(pdDfMovieIdMap, left_on=\"movieId\", right_on=\"oldMovieId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDfRatings3.drop(labels=[\"oldMovieId\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDfRatings3 = pdDfRatings3.merge(pdDfUserIdMap, left_on=\"userId\", right_on=\"oldUserId\")\n",
    "pdDfRatings3.drop(labels=[\"oldUserId\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDfTraining, pdDfTesting = train_test_split(pdDfRatings3, test_size=0.1, shuffle=True, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDfValidation, pdDfTesting = train_test_split(pdDfTesting, test_size=0.5, shuffle=True, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nUsers = pdDfRatings3.userId.unique().shape[0]\n",
    "g_nMovies = pdDfRatings3.movieId.unique().shape[0]\n",
    "g_nRatings = pdDfRatings3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparrayRatingsTraining = np.zeros((g_nUsers, g_nMovies))\n",
    "for onamedtuple in pdDfTraining.itertuples():\n",
    "    nparrayRatingsTraining[onamedtuple[6] - 1, onamedtuple[5] - 1] = onamedtuple[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparrayRatingsTesting = np.zeros((g_nUsers, g_nMovies))\n",
    "for onamedtuple in pdDfValidation.itertuples():\n",
    "    nparrayRatingsTesting[onamedtuple[6] - 1, onamedtuple[5] - 1] = onamedtuple[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparrayRatingsValidation = np.zeros((g_nUsers, g_nMovies))\n",
    "for onamedtuple in pdDfValidation.itertuples():\n",
    "    nparrayRatingsValidation[onamedtuple[6] - 1, onamedtuple[5] - 1] = onamedtuple[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparrayLabels = nparrayRatingsValidation[nparrayRatingsValidation.nonzero()].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparrayPreds = np.zeros((g_nUsers, g_nMovies))\n",
    "i = 0\n",
    "for nparray in nparrayRatingsValidation:\n",
    "    nparrayPreds[i] = np.mean(nparray[nparray > 0]) if not nparray.size else 0\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparrayPreds = nparrayPreds[nparrayRatingsValidation.nonzero()].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "fMeanSquareErr = mean_squared_error(nparrayPreds, nparrayLabels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
