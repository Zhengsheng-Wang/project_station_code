{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# The input parameter representing part here is on behalf of  all parts ahead of it and itself \n",
    "def fn_getSamples(bPositive, strPart, strJobsDir):\n",
    "    npNArrSamplesArrays = []\n",
    "    for name in os.listdir(strJobsDir):\n",
    "        strJobDir = os.path.join(strJobsDir, name)\n",
    "        npNArrJobSamples = fn_getSamplesFromAJob(bPositive, strPart, strJobDir)\n",
    "        npNArrSamplesArrays.append(npNArrJobSamples)\n",
    "    npNArrTotalSamples = np.concatenate(npNArrSamplesArrays, axis = 0)\n",
    "    return npNArrTotalSamples\n",
    "def fn_getSamplesFromAJob(bPositive, strPart, strJobDir):\n",
    "    npNArrSamples = []\n",
    "    strPosOrNegDir = \"\"\n",
    "    if bPositive:\n",
    "        strPosOrNegDir = os.path.join(strJobDir, \"samples/positive\")\n",
    "    else:\n",
    "        strPosOrNegDir = os.path.join(strJobDir, \"samples/negtive\")\n",
    "        \n",
    "    strSectionsDir = os.path.join(strPosOrNegDir, \"sections\")\n",
    "    strSamplesDir = os.path.join(strSectionsDir, strPart)\n",
    "    strSamplesFile = os.path.join(strSamplesDir, \"samples.npy\")\n",
    "    npNArrSamples = np.load(strSamplesFile)\n",
    "                \n",
    "    return npNArrSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3310, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npNArrSamples = fn_getSamples(True, \"input_carrierlock\", \"jobs\")\n",
    "npNArrSamples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "g_nBatchSize = 5\n",
    "\n",
    "def fn_DLoss(tensorTrue, tensorPred):\n",
    "        tensorOnes, tensorReal = tensorTrue[:g_nBatchSize, :], tensorPred[:g_nBatchSize, :]\n",
    "        tensorZeros, tensorFake = tensorTrue[g_nBatchSize:, :], tensorPred[g_nBatchSize:, :]\n",
    "        tensorLossReal = math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = tensorReal, labels = tensorOnes), 1)\n",
    "        tensorLossFake = math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = tensorFake, labels = tensorZeros), 1)\n",
    "        tensorLoss = tensorLossReal + tensorLossFake\n",
    "        return math.reduce_mean(tensorLoss)  \n",
    "def fn_GLoss(tensorTrue, tensorPred):\n",
    "    tensorLoss = math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = tensorPred, labels = tensorTrue), 1)\n",
    "    return math.reduce_mean(tensorLoss)\n",
    "\n",
    "class GANOperator(object):\n",
    "    \"\"\"\n",
    "        specify the number of features to be dealt by gan\n",
    "    \"\"\"\n",
    "    def __init__(self, nLatentDim, nHiddenDimG, nHiddenDimD, nBatchSize, nFeatures):\n",
    "        self.nLatentDim = nLatentDim\n",
    "        self.nHiddenDimG = nHiddenDimG\n",
    "        self.nHiddenDimD = nHiddenDimD\n",
    "        self.nBatchSize = nBatchSize\n",
    "        self.nFeatures = nFeatures\n",
    "        \n",
    "        self.oSeqGenerator = None\n",
    "        self.oSeqDiscriminator = None\n",
    "        self.fn_makeGenerator()\n",
    "        self.fn_makeDiscriminator()\n",
    "        self.fn_makeDiscriminatorModel()\n",
    "        self.fn_makeAdversariaModel()\n",
    "        \n",
    "    def fn_makeGenerator(self):\n",
    "        if self.oSeqGenerator:\n",
    "            return self.oSeqGenerator\n",
    "        self.oSeqGenerator = Sequential()\n",
    "        self.oSeqGenerator.add(Dense(self.nHiddenDimG, activation = tf.keras.activations.tanh))\n",
    "        self.oSeqGenerator.add(Dense(self.nFeatures, activation = tf.keras.activations.tanh))\n",
    "    def fn_makeDiscriminator(self):\n",
    "        if self.oSeqDiscriminator:\n",
    "            return self.oSeqDiscriminator\n",
    "        self.oSeqDiscriminator = Sequential()\n",
    "        self.oSeqDiscriminator.add(Dense(self.nHiddenDimD, activation = tf.keras.activations.tanh))\n",
    "        self.oSeqDiscriminator.add(Dense(1, activation = tf.keras.activations.sigmoid))\n",
    "    \n",
    "    def fn_makeDiscriminatorModel(self):\n",
    "        self.oSeqDiscriminatorModel = Sequential()\n",
    "        self.oSeqDiscriminatorModel.add(self.oSeqDiscriminator)\n",
    "        oOptimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "        self.oSeqDiscriminatorModel.compile(loss=fn_DLoss, optimizer=oOptimizer, metrics=[\"accuracy\"])\n",
    "    def fn_makeAdversariaModel(self):\n",
    "        self.oSeqAdversarialModel = Sequential()\n",
    "        self.oSeqAdversarialModel.add(self.oSeqGenerator)\n",
    "        self.oSeqAdversarialModel.add(self.oSeqDiscriminator)\n",
    "        self.oSeqDiscriminator.trainable = False\n",
    "        oOptimer = tf.optimizers.Adam()\n",
    "        self.oSeqAdversarialModel.compile(loss = fn_GLoss, optimizer=oOptimer, metrics=[\"accuracy\"])\n",
    "    \n",
    "    def fn_train(self, npNArrSamples, nEpochs = 2000):\n",
    "        for epoch in range(nEpochs):\n",
    "            npNArrTrueSamples = npNArrSamples[np.random.randint(0, npNArrSamples.shape[0], size = self.nBatchSize), :]\n",
    "            npNoise = np.random.uniform(-1.0, 1.0, size = [self.nBatchSize, self.nLatentDim])\n",
    "            npNArrFake = self.oSeqGenerator.predict(npNoise)\n",
    "            npNArrX = np.concatenate((npNArrTrueSamples, npNArrFake))\n",
    "            npNArrY = np.ones([2 * self.nBatchSize, 1])\n",
    "            npNArrY[self.nBatchSize:, :] = 0\n",
    "            fLossD = self.oSeqDiscriminatorModel.train_on_batch(npNArrX, npNArrY)\n",
    "            \n",
    "            npNArrY = np.ones([self.nBatchSize, 1])\n",
    "            npNoise = np.random.uniform(-1.0, 1.0, size = [self.nBatchSize, self.nLatentDim])\n",
    "            fLossA = self.oSeqAdversarialModel.train_on_batch(npNoise, npNArrY)\n",
    "\n",
    "            strMsg = \"%d: [D loss: %f, acc: %f]\" % (epoch, fLossD[0], fLossD[1])\n",
    "            strMsg = \"%s [A loss: %f, acc: %f]\" % (strMsg, fLossA[0], fLossA[1])\n",
    "            print(strMsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "oGANOperator = GANOperator(nLatentDim=8, nHiddenDimG=16, nHiddenDimD=16, nBatchSize=g_nBatchSize, \n",
    "                           nFeatures=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 1.452051, acc: 0.400000] [A loss: 0.490367, acc: 0.200000]\n",
      "1: [D loss: 1.434356, acc: 0.500000] [A loss: 0.474354, acc: 0.400000]\n",
      "2: [D loss: 1.459817, acc: 0.400000] [A loss: 0.508515, acc: 0.000000]\n",
      "3: [D loss: 1.454618, acc: 0.500000] [A loss: 0.449057, acc: 0.800000]\n",
      "4: [D loss: 1.487272, acc: 0.300000] [A loss: 0.479009, acc: 0.400000]\n",
      "5: [D loss: 1.391144, acc: 0.800000] [A loss: 0.503868, acc: 0.200000]\n",
      "6: [D loss: 1.450166, acc: 0.500000] [A loss: 0.478879, acc: 0.400000]\n",
      "7: [D loss: 1.487112, acc: 0.400000] [A loss: 0.463928, acc: 0.800000]\n",
      "8: [D loss: 1.436899, acc: 0.500000] [A loss: 0.446789, acc: 0.600000]\n",
      "9: [D loss: 1.470820, acc: 0.300000] [A loss: 0.479095, acc: 0.400000]\n"
     ]
    }
   ],
   "source": [
    "oGANOperator.fn_train(npNArrSamples, nEpochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
