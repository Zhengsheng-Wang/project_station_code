{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs _Building_ and _Training_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess training samples of a section\n",
    "\n",
    "Construct MinMaxScaler using training set, then use this MinMaxScaler transform training set and test set putting\n",
    "\n",
    "them into corresponding \"preprocessed\" directory. \n",
    "\n",
    "After preprocessing, the structure of jobs directory would be like below:\n",
    "\n",
    "- JOBXXXX\n",
    "    - status\n",
    "        - parts\n",
    "            - part\n",
    "                - section\n",
    "                    - samples\n",
    "                        - normal\n",
    "                            - train\n",
    "                                - status.csv\n",
    "                                - preprocessed\n",
    "                                    - status.csv\n",
    "                            - test\n",
    "                                - status.csv\n",
    "                                - preprocessed\n",
    "                                    - status.csv\n",
    "        - raw\n",
    "        - valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_makeMinMaxScaler(strPart, strSection, strJobsDir):\n",
    "    oMinMaxScaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    pdDfSectionsTrainSamples = fn_getSectionsTrainSamples(strPart, strSection, strJobsDir)\n",
    "    oMinMaxScaler.fit(pdDfSectionsTrainSamples.values)\n",
    "    return oMinMaxScaler\n",
    "def fn_getSectionsTrainSamples(strPart, strSection, strJobsDir):\n",
    "    listPdDfSectionTrainSamples = []\n",
    "    for job in os.listdir(strJobsDir):\n",
    "        strJobDir = os.path.join(strJobsDir, job)\n",
    "        listStrStatusDirs = [os.path.join(strJobDir, name) for name in os.listdir(strJobDir) if \"Demod\" in name]\n",
    "        for strStatusDir in listStrStatusDirs:\n",
    "            strSectionTrainSamplesFile = os.path.join(strStatusDir, \"parts/\" + strPart + \"/\" + strSection\n",
    "                                                    + \"/samples/normal/train/samples.csv\")\n",
    "            pdDfSectionTrainSamples = pd.read_csv(strSectionTrainSamplesFile)\n",
    "            listPdDfSectionTrainSamples.append(pdDfSectionTrainSamples)\n",
    "    pdDfSectionsTrainSamples = pd.concat(listPdDfSectionTrainSamples)\n",
    "    return pdDfSectionsTrainSamples\n",
    "        \n",
    "def fn_preprocessSectionTrainSamples(strPart, strSection, oMinMaxScaler, strJobsDir):\n",
    "    for job in os.listdir(strJobsDir):\n",
    "        strJobDir = os.path.join(strJobsDir, job)\n",
    "        listStrStatusDirs = [os.path.join(strJobDir, name) for name in os.listdir(strJobDir) if \"Demod\" in name]\n",
    "        for strStatusDir in listStrStatusDirs:\n",
    "            strSectionTrainSamplesFile = os.path.join(strStatusDir, \"parts/\" + strPart + \"/\" + strSection +\n",
    "                                                    \"/samples/normal/train/samples.csv\")\n",
    "            pdDfSectionTrainSamples = pd.read_csv(strSectionTrainSamplesFile)\n",
    "            strPreprocessedSectionTrainSamplesDir = os.path.join(os.path.split(strSectionTrainSamplesFile)[0], \"preprocessed\")\n",
    "            if os.path.exists(strPreprocessedSectionTrainSamplesDir):\n",
    "                shutil.rmtree(strPreprocessedSectionTrainSamplesDir)\n",
    "            os.mkdir(strPreprocessedSectionTrainSamplesDir)\n",
    "            \n",
    "            \"\"\"\n",
    "            Since empty section train samples will cause a error in oMinMaxScaler, check it first.\n",
    "            If section train samples is emtpy, the corresponding preprocessed directory contains \n",
    "            an empty dataframe with same columns.\n",
    "            \"\"\"\n",
    "            if not pdDfSectionTrainSamples.empty:\n",
    "                pdDfPreprocessedSectionTrainSamples = \\\n",
    "                pd.DataFrame(data = oMinMaxScaler.transform(pdDfSectionTrainSamples.values), \n",
    "                             columns = pdDfSectionTrainSamples.columns)\n",
    "            else:\n",
    "                pdDfPreprocessedSectionTrainSamples = \\\n",
    "                pd.DataFrame(columns=pdDfSectionTrainSamples.columns)\n",
    "                \n",
    "            strPreprocessedSectionTrainSamplesFile = os.path.join(strPreprocessedSectionTrainSamplesDir, \n",
    "                                                                 \"samples.csv\")\n",
    "            pdDfPreprocessedSectionTrainSamples.to_csv(strPreprocessedSectionTrainSamplesFile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_splitCsvLine(strLine):\n",
    "    tensorDefs = [0.] * g_nFeatures\n",
    "    tensorRecord = tf.io.decode_csv(strLine, record_defaults=tensorDefs)\n",
    "    tensorX = tf.stack(tensorRecord)\n",
    "    return tensorX\n",
    "def fn_getRealDataset(strPart, strSection, strJobsDir, \n",
    "                      nReaders = 5, nReadThreads = 5, nParseTreads = 5, nShuffleBufferSize = 1000):\n",
    "    oDataset = tf.data.Dataset.list_files(strJobsDir + \"/*/*/parts/\" + strPart + \"/\"\n",
    "                                         + strSection + \"/samples/normal/train/preprocessed/samples.csv\")\n",
    "    oDataset = oDataset.interleave(lambda strSamplesFile: tf.data.TextLineDataset(strSamplesFile).skip(1), cycle_length=nReaders, \n",
    "                                  num_parallel_calls=nReadThreads)\n",
    "    oDataset = oDataset.map(fn_splitCsvLine, nParseTreads)\n",
    "    oDataset = oDataset.shuffle(nShuffleBufferSize)\n",
    "    return oDataset.batch(g_nBatchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "oMinMaxScalerInput = fn_makeMinMaxScaler(\"framelock\", \"input\", \"../jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_preprocessSectionTrainSamples(\"framelock\", \"input\", oMinMaxScaler, \"../jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set global hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nBatchSize = 5\n",
    "g_nFeatures = 3\n",
    "oDatasetFramelockInput = fn_getRealDataset(\"framelock\", \"input\", \"../jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def fn_makeDiscriminatorTrainable(self):\\n        self.oSeqDiscriminatorModel = tf.keras.Sequential()\\n        self.oSeqDiscriminatorModel.add(self.oSeqDiscriminator)\\n        oOptimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\\n        self.oSeqDiscriminatorModel.compile(loss=self.fn_lossD, optimizer=oOptimizer)\\n    def fn_makeAdversariaTrainingModel(self):\\n        self.oSeqAdversarialModel = tf.keras.Sequential()\\n        self.oSeqAdversarialModel.add(self.oSeqGenerator)\\n        self.oSeqAdversarialModel.add(self.oSeqDiscriminator)\\n        oOptimizer = tf.optimizers.Adam(learning_rate=0.01)\\n        self.oSeqDiscriminator.trainable = False\\n        self.oSeqAdversarialModel.compile(loss = self.fn_lossA, optimizer=oOptimizer)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InputGAN(object):\n",
    "    def __init__(self):\n",
    "        #self.fn_lossD = tf.losses.binary_crossentropy\n",
    "        #self.fn_lossA = tf.losses.binary_crossentropy\n",
    "        #self.fn_metricD = tf.metrics.binary_accuracy\n",
    "        #self.fn_metricA = tf.metrics.binary_accuracy\n",
    "        \n",
    "        self.fn_makeGenerator()\n",
    "        self.fn_makeDiscriminator()\n",
    "        #self.fn_makeDiscriminatorTrainingModel()\n",
    "        #self.fn_makeAdversariaTrainingModel()\n",
    "        \n",
    "    def fn_makeGenerator(self):\n",
    "        self.oSeqGe = tf.keras.Sequential()\n",
    "        self.oSeqGe.add(tf.keras.layers.Dense(g_nFeatures, activation = tf.keras.activations.relu))\n",
    "        self.oSeqGe.add(tf.keras.layers.Dense(2))\n",
    "        \n",
    "        self.oSeqGd = tf.keras.Sequential()\n",
    "        self.oSeqGd.add(tf.keras.layers.Dense(2, activation = tf.keras.activations.relu))\n",
    "        self.oSeqGd.add(tf.keras.layers.Dense(g_nFeatures, activation = tf.keras.activations.sigmoid))\n",
    "        \n",
    "        self.oSeqGe1 = tf.keras.models.clone_model(self.oSeqGe)\n",
    "        \n",
    "        self.oSeqG = tf.keras.Sequential([\n",
    "            self.oSeqGe,\n",
    "            self.oSeqGd,\n",
    "            self.oSeqGe1\n",
    "        ])\n",
    "    def fn_makeDiscriminator(self):\n",
    "        self.oSeqDInner = tf.keras.Sequential()\n",
    "        self.oSeqDInner.add(tf.keras.layers.Dense(2, activation = tf.keras.activations.relu))\n",
    "        \n",
    "        self.oSeqDOutput = tf.keras.Sequential()\n",
    "        self.oSeqDOutput.add(tf.keras.layers.Dense(1, activation = tf.keras.activations.sigmoid))\n",
    "    \n",
    "        self.oSeqD = tf.keras.Sequential([\n",
    "            self.oSeqDInner,\n",
    "            self.oSeqDOutput\n",
    "        ])\n",
    "\"\"\"\n",
    "    def fn_makeDiscriminatorTrainable(self):\n",
    "        self.oSeqDiscriminatorModel = tf.keras.Sequential()\n",
    "        self.oSeqDiscriminatorModel.add(self.oSeqDiscriminator)\n",
    "        oOptimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        self.oSeqDiscriminatorModel.compile(loss=self.fn_lossD, optimizer=oOptimizer)\n",
    "    def fn_makeAdversariaTrainingModel(self):\n",
    "        self.oSeqAdversarialModel = tf.keras.Sequential()\n",
    "        self.oSeqAdversarialModel.add(self.oSeqGenerator)\n",
    "        self.oSeqAdversarialModel.add(self.oSeqDiscriminator)\n",
    "        oOptimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "        self.oSeqDiscriminator.trainable = False\n",
    "        self.oSeqAdversarialModel.compile(loss = self.fn_lossA, optimizer=oOptimizer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "oInputGAN = InputGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "strPrefix = time.strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "oSummaryWriterInputDLoss = tf.summary.create_file_writer(\"logs/input/\" + strPrefix + \"D\")\n",
    "oSummaryWriterInputALoss = tf.summary.create_file_writer(\"logs/input/\" + strPrefix + \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "oOptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential_2 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-284876b059ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mtensorGLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorApperantLoss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtensorLatentLoss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtensorFeatureLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mlistGGradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moGradientTapeG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorGLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moInputGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moSeqG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistGGradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moInputGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moSeqG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moSummaryWriterInputALoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1715\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m     return self._dedup_weights(\n\u001b[1;32m    545\u001b[0m         trackable_layer_utils.gather_trainable_weights(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1421\u001b[0m                        \u001b[0;34m'Weights are created when the Model is first called on '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                        \u001b[0;34m'inputs or `build()` is called with an `input_shape`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1423\u001b[0;31m                        self.name)\n\u001b[0m\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_graph_network_add_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbolic_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Weights for model sequential_2 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "nStep = 0\n",
    "for nEphoch in range(10):\n",
    "    for tensorBatch in oDatasetFramelockInput:\n",
    "        # Train discriminator\n",
    "        with tf.GradientTape() as oGradientTapeD:\n",
    "            # Generate fake samples\n",
    "            tensorEncodedSamples = oInputGAN.oSeqGe(tensorBatch)\n",
    "            tensorFakeSamples = oInputGAN.oSeqGd(tensorEncodedSamples)\n",
    "        \n",
    "            tensorTrainingSamples = tf.concat([tensorBatch, tensorFakeSamples], axis=0)\n",
    "            tensorPreds = oInputGAN.oSeqD(tensorTrainingSamples)\n",
    "            tensorLabels = tf.constant([[1.]] * g_nBatchSize + [[0.]] * g_nBatchSize)\n",
    "            tensorDLoss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tensorLabels, tensorPreds))\n",
    "        listDGradients = oGradientTapeD.gradient(tensorDLoss, oInputGAN.oSeqD.trainable_variables)\n",
    "        oOptimizer.apply_gradients(zip(listDGradients, oInputGAN.oSeqD.trainable_variables))\n",
    "        with oSummaryWriterInputDLoss.as_default():\n",
    "            tf.summary.scalar(\"D_loss\", tensorDLoss, nStep)\n",
    "        \n",
    "        # Train genertor\n",
    "        with tf.GradientTape() as oGradientTapeG:\n",
    "            # Apperant loss\n",
    "            tensorApperantLoss = tf.reduce_mean(tf.losses.mean_absolute_error(tensorBatch, tensorFakeSamples))\n",
    "            \n",
    "            # Latent loss\n",
    "            tensorEncoded1Samples = oInputGAN.oSeqGe1(tensorFakeSamples)\n",
    "            tensorLatentLoss = \\\n",
    "            tf.reduce_mean(tf.losses.mean_squared_error(tensorEncodedSamples, tensorEncoded1Samples))\n",
    "            \n",
    "            # Feature matching loss\n",
    "            tensorPredFeatures = oInputGAN.oSeqDInner(tensorFakeSamples)\n",
    "            tensorTrueFeatures = oInputGAN.oSeqDInner(tensorBatch)\n",
    "            tensorFeatureLoss = tf.reduce_mean(tf.losses.mean_squared_error(tensorPredFeatures, tensorTrueFeatures))\n",
    "            \n",
    "            tensorGLoss = tensorApperantLoss + tensorLatentLoss + tensorFeatureLoss\n",
    "        listGGradients = oGradientTapeG.gradient(tensorGLoss, oInputGAN.oSeqG.trainable_variables)\n",
    "        oOptimizer.apply_gradients(zip(listGGradients, oInputGAN.oSeqG.trainable_variables))\n",
    "        with oSummaryWriterInputALoss.as_default():\n",
    "            tf.summary.scalar(\"A_loss\", tensorGLoss, nStep)\n",
    "            \n",
    "        nStep = nStep + 1\n",
    "            \n",
    "        \"\"\"\n",
    "        tensorNoise = tf.random.normal(shape=[g_nBatchSize,  g_nCodingSize])\n",
    "        tensorGeneratedSamples = oInputGAN.oSeqGenerator(tensorNoise)\n",
    "        tensorRealAndFakeSamples = tf.concat([tensorBatch, tensorGeneratedSamples], axis=0)\n",
    "        tensorLabels = tf.constant([[1.]] * g_nBatchSize + [[0.]] * g_nBatchSize)\n",
    "        fDLoss = oInputGAN.oSeqDiscriminatorModel.train_on_batch(tensorRealAndFakeSamples, tensorLabels)\n",
    "        with oSummaryWriterInputDLoss.as_default():\n",
    "            tf.summary.scalar(\"discriminator_loss\", fDLoss, nStep)\n",
    "        '''\n",
    "        with oSummaryWriterInputDAcc.as_default():\n",
    "            tf.summary.scalar(\"discriminator_acc\", fDAcc, nStep)\n",
    "        '''\n",
    "        tensorNoise = tf.random.normal(shape=[g_nBatchSize,  g_nCodingSize])\n",
    "        tensorLabels = tf.constant([[1.]] * g_nBatchSize)\n",
    "        fALoss = oInputGAN.oSeqAdversarialModel.train_on_batch(tensorNoise, tensorLabels)\n",
    "        with oSummaryWriterInputALoss.as_default():\n",
    "            tf.summary.scalar(\"adversarial_loss\", fALoss, nStep)\n",
    "        '''\n",
    "        with oSummaryWriterInputAAcc.as_default():\n",
    "            tf.summary.scalar(\"adversarial_acc\", fAAcc, nStep)\n",
    "        '''\n",
    "        nStep += 1\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=28609693, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[-0.07572332, -0.94126445,  0.08374636],\n",
       "       [-0.5398398 , -0.97281134,  1.2739346 ]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal(shape = [2, 3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
